import streamlit as st
import pandas as pd
import numpy as np
import joblib
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
import io
from scipy import stats
import chardet
from sklearn.svm import SVR
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams  
from sklearn.utils import resample
from scipy.interpolate import interp1d, interp2d, griddata, Rbf
from sklearn.neighbors import NearestNeighbors
from itertools import combinations
from scipy.optimize import curve_fit
import base64
from pathlib import Path


rcParams['font.sans-serif'] = ['SimHei']   
rcParams['axes.unicode_minus'] = False
# è®¾ç½®é¡µé¢
st.set_page_config(page_title="è–„è†œç”Ÿé•¿å‚æ•°æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ", layout="wide")

# åº”ç”¨æ ‡é¢˜
st.title("ğŸŒ± è–„è†œç”Ÿé•¿å‚æ•°æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ")
st.markdown("ä½¿ç”¨æœºå™¨å­¦ä¹ åˆ†æè–„è†œç”Ÿé•¿å‚æ•°ä¸è–„è†œç‰¹å¾ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶é¢„æµ‹æœ€ä¼˜å·¥è‰ºå‚æ•°")

# åˆå§‹åŒ–session state
if 'model' not in st.session_state:
    st.session_state.model = None
if 'data' not in st.session_state:
    st.session_state.data = None
if 'interpolated_data' not in st.session_state:
    st.session_state.interpolated_data = None
if 'interp_accum' not in st.session_state:
    st.session_state.interp_accum = pd.DataFrame()
if 'features' not in st.session_state:
    st.session_state.features = []
if 'target' not in st.session_state:
    st.session_state.target = None
if 'feature_types' not in st.session_state:
    st.session_state.feature_types = {}
if 'trained' not in st.session_state:
    st.session_state.trained = False
if 'substrate_option' not in st.session_state:
    st.session_state.substrate_option = "æ‰€æœ‰åŸºç‰‡"
if 'selected_substrate' not in st.session_state:
    st.session_state.selected_substrate = None
if 'augmentation_method' not in st.session_state:
    st.session_state.augmentation_method = "æ— "
if 'augmentation_factor' not in st.session_state:
    st.session_state.augmentation_factor = 1.0

# è¯»å–æœ¬åœ°å›¾ç‰‡å¹¶è½¬æ¢ä¸º base64
def img_to_base64(img_path):
    return base64.b64encode(Path(img_path).read_bytes()).decode()

# æ£€æµ‹æ–‡ä»¶ç¼–ç çš„å‡½æ•°
def detect_encoding(file_content):
    result = chardet.detect(file_content)
    return result['encoding']

# æ•°æ®å¢å¼ºå‡½æ•°
def augment_data(X, y, method="æ— ", factor=1.0, feature_types=None):
    """
    æ•°æ®å¢å¼ºå‡½æ•°
    method: å¢å¼ºæ–¹æ³•ï¼Œå¯é€‰ "æ— ", "é«˜æ–¯å™ªå£°", "è‡ªåŠ©æ³•", "æ’å€¼æ³•", "SMOTEå¼"
    factor: å¢å¼ºå› å­ï¼Œ1.0è¡¨ç¤ºä¸å¢å¼ºï¼Œ2.0è¡¨ç¤ºæ•°æ®é‡ç¿»å€
    """
    if method == "æ— " or factor <= 1.0:
        return X, y
    
    n_samples = len(X)
    n_new = int(n_samples * (factor - 1.0))
    
    if n_new <= 0:
        return X, y
    
    X_new = X.copy()
    y_new = y.copy()
    
    if method == "é«˜æ–¯å™ªå£°":
        # æ·»åŠ é«˜æ–¯å™ªå£°
        numeric_cols = [col for col in X.columns if feature_types.get(col) == "æ•°å€¼å‹"]
        categorical_cols = [col for col in X.columns if feature_types.get(col) == "åˆ†ç±»å‹"]
        
        for _ in range(n_new):
            # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
            idx = np.random.randint(0, n_samples)
            sample = X.iloc[idx].copy()
            
            # å¯¹æ•°å€¼å‹ç‰¹å¾æ·»åŠ å™ªå£°
            for col in numeric_cols:
                std = X[col].std() * 0.05  # å™ªå£°æ ‡å‡†å·®ä¸ºåŸç‰¹å¾æ ‡å‡†å·®çš„5%
                sample[col] += np.random.normal(0, std)
            
            X_new = pd.concat([X_new, sample.to_frame().T], ignore_index=True)
            y_new = pd.concat([y_new, pd.Series([y.iloc[idx]])], ignore_index=True)
    
    elif method == "è‡ªåŠ©æ³•":
        # è‡ªåŠ©æ³•é‡é‡‡æ ·
        for _ in range(n_new):
            idx = np.random.randint(0, n_samples)
            X_new = pd.concat([X_new, X.iloc[[idx]]], ignore_index=True)
            y_new = pd.concat([y_new, pd.Series([y.iloc[idx]])], ignore_index=True)
    
    elif method == "æ’å€¼æ³•":
        # åœ¨æ•°å€¼å‹ç‰¹å¾ä¹‹é—´è¿›è¡Œæ’å€¼
        numeric_cols = [col for col in X.columns if feature_types.get(col) == "æ•°å€¼å‹"]
        categorical_cols = [col for col in X.columns if feature_types.get(col) == "åˆ†ç±»å‹"]
        
        for _ in range(n_new):
            # éšæœºé€‰æ‹©ä¸¤ä¸ªä¸åŒçš„æ ·æœ¬
            idx1, idx2 = np.random.choice(n_samples, 2, replace=False)
            alpha = np.random.random()
            
            # åˆ›å»ºæ–°æ ·æœ¬
            new_sample = X.iloc[idx1].copy()
            
            # å¯¹æ•°å€¼å‹ç‰¹å¾è¿›è¡Œæ’å€¼
            for col in numeric_cols:
                val1 = X.iloc[idx1][col]
                val2 = X.iloc[idx2][col]
                new_sample[col] = alpha * val1 + (1 - alpha) * val2
            
            # å¯¹åˆ†ç±»ç‰¹å¾ï¼Œéšæœºé€‰æ‹©å…¶ä¸­ä¸€ä¸ªæ ·æœ¬çš„å€¼
            for col in categorical_cols:
                if np.random.random() > 0.5:
                    new_sample[col] = X.iloc[idx1][col]
                else:
                    new_sample[col] = X.iloc[idx2][col]
            
            # å¯¹ç›®æ ‡å˜é‡è¿›è¡Œæ’å€¼
            new_target = alpha * y.iloc[idx1] + (1 - alpha) * y.iloc[idx2]
            
            X_new = pd.concat([X_new, new_sample.to_frame().T], ignore_index=True)
            y_new = pd.concat([y_new, pd.Series([new_target])], ignore_index=True)
    
    elif method == "SMOTEå¼":
        # ç±»ä¼¼SMOTEçš„æ–¹æ³•ï¼Œç”¨äºå›å½’é—®é¢˜
        numeric_cols = [col for col in X.columns if feature_types.get(col) == "æ•°å€¼å‹"]
        categorical_cols = [col for col in X.columns if feature_types.get(col) == "åˆ†ç±»å‹"]
        
        # ä½¿ç”¨Kè¿‘é‚»
        n_neighbors = min(5, n_samples - 1)
        if n_neighbors > 0:
            knn = NearestNeighbors(n_neighbors=n_neighbors)
            knn.fit(X[numeric_cols])
            
            for _ in range(n_new):
                # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
                idx = np.random.randint(0, n_samples)
                
                # æ‰¾åˆ°æœ€è¿‘é‚»
                distances, indices = knn.kneighbors([X.iloc[idx][numeric_cols]], n_neighbors+1)
                
                # éšæœºé€‰æ‹©ä¸€ä¸ªè¿‘é‚»ï¼ˆæ’é™¤è‡ªèº«ï¼‰
                neighbor_idx = np.random.choice(indices[0][1:])
                
                # åˆ›å»ºæ–°æ ·æœ¬
                new_sample = X.iloc[idx].copy()
                alpha = np.random.random()
                
                # å¯¹æ•°å€¼å‹ç‰¹å¾è¿›è¡Œæ’å€¼
                for col in numeric_cols:
                    val1 = X.iloc[idx][col]
                    val2 = X.iloc[neighbor_idx][col]
                    new_sample[col] = alpha * val1 + (1 - alpha) * val2
                
                # å¯¹ç›®æ ‡å˜é‡è¿›è¡Œæ’å€¼
                new_target = alpha * y.iloc[idx] + (1 - alpha) * y.iloc[neighbor_idx]
                
                X_new = pd.concat([X_new, new_sample.to_frame().T], ignore_index=True)
                y_new = pd.concat([y_new, pd.Series([new_target])], ignore_index=True)
    
    return X_new, y_new

# æ’å€¼å‡½æ•°
def interpolate_scalar(val1, val2, alpha, method='linear'):
    # å¯¹å•å€¼çš„æ’å€¼å™¨ï¼ˆæ•°å€¼ï¼‰
    if method == 'linear':
        return (1 - alpha) * val1 + alpha * val2
    elif method == 'quadratic':
        t = alpha
        return (1 - t)**2 * val1 + 2 * (1 - t) * t * ((val1 + val2) / 2) + t**2 * val2
    elif method == 'exponential':
        if val1 > 0 and val2 > 0:
            return val1 * (val2 / val1) ** alpha
        else:
            return (1 - alpha) * val1 + alpha * val2
    elif method == 'logarithmic':
        if val1 > 0 and val2 > 0:
            return val1 * np.exp(alpha * np.log(val2 / val1))
        else:
            return (1 - alpha) * val1 + alpha * val2
    else:
        return (1 - alpha) * val1 + alpha * val2
    
def interpolate_between_points(point1, point2, feature_names, target_name, num_points=5, method='linear', feature_types=None):
    """
    åœ¨ä¸¤ç‚¹ä¹‹é—´è¿›è¡Œæ’å€¼
    """
    # å…è®¸å¤–éƒ¨ä¼ å…¥ feature_typesï¼›è‹¥æœªä¼ å…¥åˆ™å›é€€åˆ° session_state
    if feature_types is None:
        feature_types = st.session_state.feature_types if 'feature_types' in st.session_state else {}
    
    # æå–æ•°å€¼å‹ç‰¹å¾
    numeric_features = [f for f in feature_names if f != target_name and st.session_state.feature_types.get(f) == "æ•°å€¼å‹"]
    categorical_features = [f for f in feature_names if f != target_name and st.session_state.feature_types.get(f) == "åˆ†ç±»å‹"]
    
    # åˆ›å»ºæ’å€¼ç»“æœ
    interpolated_points = []
    
    # å¯¹æ¯ä¸ªæ’å€¼ç‚¹
    for i in range(num_points):
        alpha = i / (num_points - 1) if num_points > 1 else 0.5
        
        # åˆ›å»ºæ–°ç‚¹
        new_point = {}
        
        # å¯¹æ•°å€¼å‹ç‰¹å¾è¿›è¡Œæ’å€¼
        for feature in numeric_features:
            val1 = point1[feature]
            val2 = point2[feature]
            
            if method == 'linear':
                new_point[feature] = (1 - alpha) * val1 + alpha * val2
            elif method == 'quadratic':
                # äºŒæ¬¡æ’å€¼
                t = alpha
                new_point[feature] = (1 - t)**2 * val1 + 2 * (1 - t) * t * ((val1 + val2) / 2) + t**2 * val2
            elif method == 'exponential':
                # æŒ‡æ•°æ’å€¼
                if val1 > 0 and val2 > 0:
                    new_point[feature] = val1 * (val2 / val1) ** alpha
                else:
                    new_point[feature] = (1 - alpha) * val1 + alpha * val2
            elif method == 'logarithmic':
                # å¯¹æ•°æ’å€¼
                if val1 > 0 and val2 > 0:
                    new_point[feature] = val1 * np.exp(alpha * np.log(val2 / val1))
                else:
                    new_point[feature] = (1 - alpha) * val1 + alpha * val2
        
        # å¯¹åˆ†ç±»ç‰¹å¾ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªç‚¹çš„å€¼
        for feature in categorical_features:
            new_point[feature] = point1[feature]
        
        # å¯¹ç›®æ ‡å˜é‡è¿›è¡Œæ’å€¼
        val1 = point1[target_name]
        val2 = point2[target_name]
        
        if method == 'linear':
            new_point[target_name] = (1 - alpha) * val1 + alpha * val2
        elif method == 'quadratic':
            t = alpha
            new_point[target_name] = (1 - t)**2 * val1 + 2 * (1 - t) * t * ((val1 + val2) / 2) + t**2 * val2
        elif method == 'exponential':
            if val1 > 0 and val2 > 0:
                new_point[target_name] = val1 * (val2 / val1) ** alpha
            else:
                new_point[target_name] = (1 - alpha) * val1 + alpha * val2
        elif method == 'logarithmic':
            if val1 > 0 and val2 > 0:
                new_point[target_name] = val1 * np.exp(alpha * np.log(val2 / val1))
            else:
                new_point[target_name] = (1 - alpha) * val1 + alpha * val2
        
        interpolated_points.append(new_point)
    
    return interpolated_points

def visualize_xy(x, y, x_label, y_label, title, orig=None):
    fig, ax = plt.subplots(figsize=(10, 6))
    if orig is not None:
        ax.scatter(orig['x'], orig['y'], s=120, c='red', label='åŸå§‹èŠ‚ç‚¹')
    ax.plot(x, y, 'o--', alpha=0.7, label='æ’å€¼åºåˆ—')
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)
    ax.set_title(title)
    ax.grid(True, alpha=0.3)
    ax.legend()
    st.pyplot(fig)
logo_path = "logo.png"  # æ›¿æ¢ä¸ºæ‚¨çš„å›¾ç‰‡è·¯å¾„
logo_base64 = img_to_base64(logo_path)
st.sidebar.markdown(
    f'<div style="text-align: center;"><img src="data:image/png;base64,{logo_base64}" width="400"></div>', 
    unsafe_allow_html=True
)
def _load_model_from_joblib(uploaded_file):
    """
    è¯»å– joblib/pklã€‚å…¼å®¹ä¸¤ç§ä¿å­˜æ–¹å¼ï¼š
    1) dict åŒ…å« {"model":..., "target":..., "features":..., "feature_types":...}
    2) ç›´æ¥æ˜¯æ‹Ÿåˆå¥½çš„ sklearn Pipeline/Estimator
    è¿”å›: model, target, features, feature_types
    """
    obj = joblib.load(uploaded_file)
    model, target, features, feature_types = obj, None, None, None
    if isinstance(obj, dict) and "model" in obj:
        model = obj["model"]
        target = obj.get("target", None)
        features = obj.get("features", None)
        feature_types = obj.get("feature_types", None)
    return model, target, features, feature_types

def _infer_features_if_missing(model, fallback_df, known_targets=None):
    """
    å°è¯•åœ¨ç¼ºå¤± features æ—¶æ¨æ–­ã€‚ä¼˜å…ˆï¼š
    1) sklearn >=1.0 çš„ feature_names_in_
    2) ä» fallback_df å»æ‰å·²çŸ¥ targets
    """
    if hasattr(model, "feature_names_in_"):
        return list(model.feature_names_in_)
    if fallback_df is not None:
        known_targets = set(known_targets or [])
        return [c for c in fallback_df.columns if c not in known_targets]
    return []

def _infer_feature_types_if_missing(features, df, given=None):
    """
    åœ¨æœªæä¾› feature_types æ—¶ï¼Œä» df.dtype æ¨æ–­ï¼šæ•°å€¼å‹/åˆ†ç±»å‹
    """
    if given:
        return given
    ft = {}
    if df is None:
        return ft
    for f in features:
        if f in df.columns:
            ft[f] = "æ•°å€¼å‹" if str(df[f].dtype) in ["int64", "float64"] else "åˆ†ç±»å‹"
    return ft

def _sample_candidates(n, features, feature_types, param_limits):
    """
    åœ¨ç»™å®šé™åˆ¶ param_limits ä¸‹éšæœºç”Ÿæˆ n ç»„å€™é€‰å‚æ•°ã€‚
    param_limits:
      - æ•°å€¼å‹: (low, high)
      - åˆ†ç±»å‹: [opt1, opt2, ...]
    """
    rows = []
    for _ in range(n):
        row = {}
        for f in features:
            if feature_types.get(f) == "æ•°å€¼å‹":
                low, high = param_limits[f]
                row[f] = np.random.uniform(low, high)
            else:
                choices = param_limits[f]
                # é˜²å¾¡ï¼šè‹¥ç©ºåˆ—è¡¨ï¼Œå…ˆè·³è¿‡ï¼Œç¨åç”¨å…¨éƒ¨ unique å€¼å…œåº•
                row[f] = np.random.choice(choices) if len(choices) > 0 else None
        rows.append(row)
    return pd.DataFrame(rows)


def _predict_with_models(candidates_df, models_by_target, global_features=None):
    """
    ä½¿ç”¨â€œå¤šä¸ªå•è¾“å‡ºæ¨¡å‹ï¼ˆæ¯ä¸ª target ä¸€ä¸ªï¼‰â€è¿›è¡Œé¢„æµ‹ã€‚
    models_by_target: { target: {"model":..., "features":..., "feature_types":...} }
    global_features: è‹¥å•æ¨¡å‹æ¡ç›®æ ‡æœªæºå¸¦ featuresï¼Œåˆ™ç”¨æ­¤å…¨å±€ featuresã€‚
    è¿”å›ï¼špreds_dfï¼ˆåˆ—ä¸ºå„ targetï¼‰
    """
    preds = {}
    for t, pack in models_by_target.items():
        m = pack["model"]
        fts = pack.get("features")
        if fts is None or len(fts) == 0:
            # ä»æ¨¡å‹æˆ–å…¨å±€æ¨æ–­
            fts = _infer_features_if_missing(m, st.session_state.get("data"), known_targets=[t])
            if (not fts) and global_features:
                fts = list(global_features)
        # é˜²å¾¡ï¼šè‹¥ candidates ä¸å«è¯¥æ¨¡å‹å…¨éƒ¨ç‰¹å¾ï¼Œåšäº¤é›†
        use_cols = [c for c in fts if c in candidates_df.columns]
        X = candidates_df[use_cols]
        yhat = m.predict(X)
        # ç»Ÿä¸€æˆ 1D
        yhat = np.ravel(yhat)
        preds[t] = yhat
    return pd.DataFrame(preds)


def _compute_weighted_error(preds_df, target_vals, target_weights=None):
    """
    è®¡ç®—åŠ æƒ L1 è¯¯å·®ã€‚target_weights å¯ä¸º {target: weight}
    """
    if target_weights is None:
        target_weights = {t: 1.0 for t in target_vals.keys()}
    err = 0.0
    for t, val in target_vals.items():
        w = float(target_weights.get(t, 1.0))
        err += w * np.abs(preds_df[t] - val)
    return err

# ä¾§è¾¹æ å¯¼èˆª
page = st.sidebar.selectbox("å¯¼èˆª", ["å®éªŒæ•°æ®æ’å€¼å¢å¼º","æ•°æ®ä¸Šä¼ ä¸è®¾ç½®", "æ¨¡å‹è®­ç»ƒ", "å‚æ•°é¢„æµ‹","åå‘é¢„æµ‹", "ç‰¹å¾åˆ†æ", "å‚æ•°åˆ†å¸ƒåˆ†æ","æ¨¡å‹ç¨³å®šæ€§éªŒè¯"])
# -------------------- é¡µé¢ï¼šå®éªŒæ•°æ®æ’å€¼å¢å¼º --------------------
if page == "å®éªŒæ•°æ®æ’å€¼å¢å¼º":
    st.header("ğŸ”¬ å°æ ·æœ¬æ’å€¼æ„é€ å™¨ï¼ˆåŸºäºç‰©ç†ç›´è§‰çš„æ•°æ®å¢å¹¿ï¼‰")
    st.markdown(
        "é€‚ç”¨äº 5~10 ç»„å°æ ·æœ¬ï¼šä»»æ„é€‰æ‹©ä¸¤ç‚¹ï¼›"
        "å½“**å•å˜é‡**å˜åŒ–æ—¶é€‰æ‹©æ’å€¼å‡½æ•°ï¼›å½“**å¤šå˜é‡**å˜åŒ–æ—¶ï¼Œè®¾å®šå˜åŒ–é¡ºåºä¸è¿‡æ¸¡èŠ‚ç‚¹ï¼ˆå¯ç»™å®šèŠ‚ç‚¹ç›®æ ‡ç»éªŒå€¼ï¼‰ï¼Œ"
        "ä¸ºæ¯ä¸ªé˜¶æ®µåˆ†åˆ«é€‰æ‹©æ’å€¼æ–¹æ³•ä¸ç‚¹æ•°ï¼›ç»“æœå¯**ç´¯è®¡åˆ°æ’å€¼ç¼“å­˜**å¹¶**å¯¼å‡º/è¿½åŠ è‡³CSV**ã€‚"
    )

    # è¯»å–å°æ ·æœ¬æ•°æ®
    uploaded_file = st.file_uploader("ä¸Šä¼ å°æ ·æœ¬ CSV", type=['csv'])
    if uploaded_file is not None:
        try:
            file_content = uploaded_file.read()
            encoding = detect_encoding(file_content)
            uploaded_file.seek(0)
            df = pd.read_csv(uploaded_file, encoding=encoding)
            if df.empty or df.columns.str.contains('Unnamed').any():
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, encoding=encoding, header=0)

            st.session_state.data = df
            st.subheader("æ•°æ®é¢„è§ˆ")
            st.dataframe(df)
            st.caption(f"ç¼–ç : {encoding}ï¼Œå½¢çŠ¶: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")

            # é€‰æ‹©ç›®æ ‡ä¸ç‰¹å¾ï¼Œå¹¶æ ‡æ³¨ç±»å‹
            target = st.selectbox("é€‰æ‹©é¢„æµ‹ç›®æ ‡(è§‚æµ‹é‡)", df.columns.tolist(), key="interp_target")
            feature_options = [c for c in df.columns if c != target]
            selected_features = st.multiselect("é€‰æ‹©ç‰¹å¾å‚æ•°", feature_options, default=feature_options, key="interp_features")

            st.subheader("æ ‡æ³¨ç‰¹å¾ç±»å‹")
            feature_types = {}
            cols2 = st.columns(2)
            for i, f in enumerate(selected_features):
                with cols2[i % 2]:
                    default_type = "æ•°å€¼å‹" if str(df[f].dtype) in ['int64', 'float64'] else "åˆ†ç±»å‹"
                    feature_types[f] = st.selectbox(f"{f} ç±»å‹", ["æ•°å€¼å‹", "åˆ†ç±»å‹"],
                                                    index=0 if default_type == "æ•°å€¼å‹" else 1,
                                                    key=f"interp_type_{f}")
            st.session_state.feature_types = feature_types

            if len(df) < 2:
                st.error("è‡³å°‘éœ€è¦ä¸¤ä¸ªæ•°æ®ç‚¹ã€‚")
                st.stop()

            st.subheader("é€‰æ‹©ä¸¤ç‚¹")
            idx1 = st.selectbox("ç¬¬ä¸€ä¸ªç‚¹ï¼ˆç´¢å¼•ï¼‰", list(range(len(df))), format_func=lambda i: f"#{i+1}")
            idx2 = st.selectbox("ç¬¬äºŒä¸ªç‚¹ï¼ˆç´¢å¼•ï¼‰", list(range(len(df))), index=min(1, len(df)-1), format_func=lambda i: f"#{i+1}")
            if idx1 == idx2:
                st.error("è¯·é€‰æ‹©ä¸¤ä¸ªä¸åŒçš„æ•°æ®ç‚¹ã€‚")
                st.stop()

            p1 = df.iloc[idx1].to_dict()
            p2 = df.iloc[idx2].to_dict()
            # ===== ä¸¤ç‚¹è¯¦æƒ…ï¼ˆæ¨ªå‘è¡¨æ ¼ï¼‰ã€æ›¿æ¢åŸå…ˆä½¿ç”¨ st.json çš„ expander å—ã€‘=====
            with st.expander("ä¸¤ç‚¹è¯¦æƒ…ï¼ˆæ¨ªå‘è¡¨æ ¼ï¼‰", expanded=True):
                cols_show = selected_features + [target]

                # ç›´æ¥ä» df æŒ‰è¡Œåˆ‡ç‰‡ï¼Œä¿ç•™"æ¨ªå‘"ç»“æ„ï¼Œé¿å… dict.name æŠ¥é”™
                row1 = df.loc[[idx1], cols_show].copy()
                row2 = df.loc[[idx2], cols_show].copy()
                row1.index = [f"ç‚¹ #{idx1+1}"]
                row2.index = [f"ç‚¹ #{idx2+1}"]

                c1, c2 = st.columns(2)
                with c1:
                    st.markdown("**ç‚¹ 1**")
                    st.dataframe(row1, use_container_width=True)
                with c2:
                    st.markdown("**ç‚¹ 2**")
                    st.dataframe(row2, use_container_width=True)

                # å¯é€‰ï¼šåˆå¹¶å¯¹æ¯” + é«˜äº®å‡º"ä¸åŒ"çš„åˆ—ï¼ˆé»„è‰²ï¼‰
                compare_df = pd.concat([row1, row2], axis=0)

                def _style_diff(df_):
                    styles = pd.DataFrame('', index=df_.index, columns=df_.columns)
                    if len(df_) == 2:
                        a, b = df_.iloc[0], df_.iloc[1]
                        for c in df_.columns:
                            v1, v2 = a[c], b[c]
                            different = False
                            if pd.api.types.is_numeric_dtype(df_[c]):
                                if pd.isna(v1) or pd.isna(v2):
                                    different = pd.isna(v1) != pd.isna(v2)
                                else:
                                    try:
                                        different = not np.isclose(float(v1), float(v2), rtol=1e-6, atol=1e-12)
                                    except Exception:
                                        different = (v1 != v2)
                            else:
                                different = (v1 != v2)
                            if different:
                                styles.loc[:, c] = 'background-color: #ffeb3b33'  # æµ…é»„
                    return styles

                st.caption("ä¸¤ç‚¹åˆå¹¶å¯¹æ¯”ï¼ˆé»„è‰²=ä¸¤ç‚¹å–å€¼ä¸åŒï¼‰")
                try:
                    st.dataframe(compare_df.style.apply(_style_diff, axis=None), use_container_width=True)
                except Exception:
                    # è‹¥æœ¬åœ° Streamlit ç‰ˆæœ¬ä¸æ”¯æŒ Stylerï¼Œé€€åŒ–ä¸ºæ™®é€šè¡¨æ ¼
                    st.dataframe(compare_df, use_container_width=True)

            
            # æ£€æµ‹å˜åŒ–ç‰¹å¾
            changed = [f for f in selected_features if p1[f] != p2[f]]
            st.info(f"å˜åŒ–çš„ç‰¹å¾ï¼š{', '.join(changed) if changed else 'æ— '}")

            # ---------- å•å˜é‡ ----------
            if len(changed) == 1:
                st.success("âœ… å•å˜é‡å˜åŒ–ï¼Œå¯ç›´æ¥æ’å€¼")
                m1, m2, m3 = st.columns([1,1,1])
                with m1:
                    num_points = st.slider("æ’å€¼ç‚¹æ•°ï¼ˆå«ä¸¤ç«¯ï¼‰", 2, 50, 7)
                with m2:
                    method_name = st.selectbox("æ’å€¼æ–¹æ³•", ["çº¿æ€§", "äºŒæ¬¡", "æŒ‡æ•°", "å¯¹æ•°"])
                method_map = {"çº¿æ€§":"linear", "äºŒæ¬¡":"quadratic", "æŒ‡æ•°":"exponential", "å¯¹æ•°":"logarithmic"}
                method = method_map[method_name]

                if st.button("ç”Ÿæˆæ’å€¼æ•°æ®ï¼ˆå•å˜é‡ï¼‰", use_container_width=True):
                    interps = interpolate_between_points(
                        p1, p2, selected_features + [target], target,
                        num_points=num_points, method=method, feature_types=feature_types
                    )
                    interp_df = pd.DataFrame(interps)

                    st.subheader("æ’å€¼ç»“æœï¼ˆå•å˜é‡ï¼‰")
                    st.dataframe(interp_df)

                    # å¯è§†åŒ–ï¼ˆx=å”¯ä¸€å˜åŒ–ç‰¹å¾ï¼‰
                    xf = changed[0]
                    x = [d[xf] for d in interps]
                    y = [d[target] for d in interps]
                    visualize_xy(x, y, xf, target, f"{xf} â†’ {target} ({method_name})",
                                 orig={'x':[p1[xf], p2[xf]], 'y':[p1[target], p2[target]]})

                    # ç´¯è®¡åˆ°æ’å€¼ç¼“å­˜
                    st.session_state.interpolated_data = interp_df
                    st.session_state.interp_accum = pd.concat([st.session_state.interp_accum, interp_df], ignore_index=True)
                    st.success(f"å·²è¿½åŠ åˆ°æ’å€¼ç¼“å­˜ï¼Œå½“å‰ç¼“å­˜å…± {len(st.session_state.interp_accum)} è¡Œã€‚")

            # ---------- å¤šå˜é‡ ----------
            elif len(changed) >= 2:
                st.warning("âš ï¸ å¤šå˜é‡å˜åŒ–ï¼šè¯·è®¾ç½®å˜åŒ–é¡ºåºã€è¿‡æ¸¡èŠ‚ç‚¹ä¸æ¯é˜¶æ®µæ–¹æ³•ã€‚")

                # å˜åŒ–é¡ºåºï¼ˆæ’åˆ—ï¼‰
                order = st.multiselect("é€‰æ‹©å¹¶æ’åºå˜é‡å˜åŒ–é¡ºåºï¼ˆä»å…ˆåˆ°åï¼‰", changed, default=changed)
                if len(order) != len(changed):
                    st.error("è¯·æŠŠæ‰€æœ‰å‘ç”Ÿå˜åŒ–çš„ç‰¹å¾éƒ½æ”¾å…¥é¡ºåºåˆ—è¡¨ï¼Œå¹¶ä¿æŒæ— é‡å¤ã€‚")
                    st.stop()

                # è¿‡æ¸¡èŠ‚ç‚¹æ•° = n-1
                n = len(changed)
                num_nodes = n - 1
                st.caption(f"å°†ä» {tuple(p1[f] for f in order)} å˜åŒ–åˆ° {tuple(p2[f] for f in order)}ï¼Œéœ€è¦ {num_nodes} ä¸ªè¿‡æ¸¡èŠ‚ç‚¹ã€‚")
                st.info("èŠ‚ç‚¹å«ä¹‰ï¼šæŒ‰é¡ºåºä¾æ¬¡æŠŠç‰¹å¾ä»ç‚¹1çš„å€¼åˆ‡æ¢æˆç‚¹2çš„å€¼ï¼Œä¾‹å¦‚ Aâ†’Bâ†’Cï¼šèŠ‚ç‚¹1æ”¹å˜ Aï¼ŒèŠ‚ç‚¹2 å†æ”¹å˜ Bï¼Œæœ€ç»ˆç¬¬ä¸‰æ®µæ”¹å˜ Cã€‚")

                # ç”ŸæˆèŠ‚ç‚¹ï¼ˆä»…ç‰¹å¾ï¼Œä¸å« targetï¼‰
                nodes = []
                cur = p1.copy()
                for i, feat in enumerate(order):
                    # ç¬¬ i ä¸ªèŠ‚ç‚¹ï¼šfeat æ”¹æˆ p2[feat]ï¼Œå…¶å®ƒæœªå˜åŠ¨çš„ä»å–å½“å‰å€¼
                    cur = cur.copy()
                    cur[feat] = p2[feat]
                    nodes.append({f: cur[f] for f in selected_features})

                # è®©ç”¨æˆ·å¯è®¾ç½®æ¯ä¸ª**ä¸­é—´èŠ‚ç‚¹**çš„ç»éªŒç›®æ ‡å€¼ï¼ˆä¸å«èµ·ç‚¹ã€ç»ˆç‚¹ï¼‰
                st.subheader("è®¾ç½®èŠ‚ç‚¹ç›®æ ‡ç»éªŒå€¼ï¼ˆå¯é€‰ï¼‰")
                st.caption("å¦‚æœä¸å¡«ï¼Œåˆ™é»˜è®¤æŒ‰æ•´æ®µçº¿æ€§è¿‡æ¸¡ä¼°è®¡èŠ‚ç‚¹ç›®æ ‡å€¼ã€‚")
                node_targets = {}
                for i in range(num_nodes):
                    with st.expander(f"è¿‡æ¸¡èŠ‚ç‚¹ #{i+1}ï¼ˆå®Œæˆ {order[i]} çš„åˆ‡æ¢ï¼‰", expanded=False):
                        default_alpha = (i+1) / (n)  # åœ¨ 0..1 å‡åŒ€ä¼°è®¡
                        default_t = interpolate_scalar(p1[target], p2[target], default_alpha, method='linear')
                        val = st.text_input(
                            f"èŠ‚ç‚¹ #{i+1} çš„ {target} ç»éªŒå€¼",
                            value="",
                            help=f"ç•™ç©ºè¡¨ç¤ºä½¿ç”¨é»˜è®¤ä¼°è®¡å€¼ï¼ˆå½“å‰å»ºè®® {default_t:.4f}ï¼‰",
                            key=f"node_t_{i}"
                        )
                        node_targets[i] = float(val) if val.strip() != "" else float(default_t)
                st.session_state.node_targets = node_targets

                # ===== è¿‡æ¸¡èŠ‚ç‚¹çŠ¶æ€è¡¨ï¼ˆå®æ—¶ï¼‰ã€æ’åœ¨"è®¾ç½®èŠ‚ç‚¹ç›®æ ‡ç»éªŒå€¼"ä¹‹åï¼Œ"æ¯é˜¶æ®µé…ç½®"ä¹‹å‰ã€‘=====
                st.subheader("è¿‡æ¸¡èŠ‚ç‚¹çŠ¶æ€è¡¨ï¼ˆå®æ—¶ï¼‰")
                node_rows = []
                node_labels = []

                # èµ·ç‚¹
                node_rows.append({**{f: p1[f] for f in selected_features}, target: p1[target]})
                node_labels.append("èµ·ç‚¹")

                # ä¸­é—´è¿‡æ¸¡èŠ‚ç‚¹ï¼šä½¿ç”¨ä¸Šæ–¹è¾“å…¥æ¡†è®¾ç½®çš„ç»éªŒç›®æ ‡å€¼ï¼ˆè‹¥ç•™ç©ºåˆ™é‡‡ç”¨é»˜è®¤çº¿æ€§ä¼°è®¡ï¼‰
                for i in range(num_nodes):
                    r = nodes[i].copy()
                    r[target] = st.session_state.node_targets[i]
                    node_rows.append(r)
                    node_labels.append(f"è¿‡æ¸¡{i+1}")

                # ç»ˆç‚¹
                node_rows.append({**{f: p2[f] for f in selected_features}, target: p2[target]})
                node_labels.append("ç»ˆç‚¹")

                node_table = pd.DataFrame(node_rows, index=node_labels)[selected_features + [target]]

                def _style_transition(df_):
                    # é«˜äº®æ¯è¡Œç›¸å¯¹äºä¸Šä¸€è¡Œå‘ç”Ÿå˜åŒ–çš„å•å…ƒæ ¼ï¼ˆæµ…ç»¿ï¼‰
                    styles = pd.DataFrame('', index=df_.index, columns=df_.columns)
                    for i in range(1, len(df_)):
                        prev = df_.iloc[i-1]
                        cur = df_.iloc[i]
                        for c in df_.columns:
                            changed_cell = False
                            if pd.api.types.is_numeric_dtype(df_[c]):
                                v1, v2 = prev[c], cur[c]
                                if pd.isna(v1) or pd.isna(v2):
                                    changed_cell = pd.isna(v1) != pd.isna(v2)
                                else:
                                    try:
                                        changed_cell = not np.isclose(float(v1), float(v2), rtol=1e-6, atol=1e-12)
                                    except Exception:
                                        changed_cell = (v1 != v2)
                            else:
                                changed_cell = (prev[c] != cur[c])

                            if changed_cell:
                                styles.iloc[i, df_.columns.get_loc(c)] = 'background-color: #87CEFA'  # æµ…ç»¿
                    return styles

                try:
                    st.dataframe(node_table.style.apply(_style_transition, axis=None), use_container_width=True)
                except Exception:
                    st.dataframe(node_table, use_container_width=True)

                st.caption("æµ…ç»¿è¡¨ç¤ºè¯¥èŠ‚ç‚¹ç›¸å¯¹äºä¸Šä¸€èŠ‚ç‚¹åœ¨è¯¥å­—æ®µå‘ç”Ÿäº†å˜åŒ–ï¼›ä¸Šæ–¹è¾“å…¥çš„è§‚æµ‹é‡ç»éªŒå€¼ä¼šå³æ—¶åæ˜ åˆ°è¡¨æ ¼ä¸­ã€‚")


                st.subheader("æ¯é˜¶æ®µé…ç½®ï¼ˆæ–¹æ³• & æ’å€¼ç‚¹æ•°ï¼‰")
                stage_cfg = {}
                stage_cols = st.columns(3)
                for s in range(n):
                    with stage_cols[s % 3]:
                        st.markdown(f"**é˜¶æ®µ {s+1}ï¼šæ”¹å˜ {order[s]}**")
                        mname = st.selectbox(f"æ–¹æ³•ï¼ˆé˜¶æ®µ {s+1}ï¼‰", ["çº¿æ€§", "äºŒæ¬¡", "æŒ‡æ•°", "å¯¹æ•°"], key=f"stage_m_{s}")
                        pts = st.number_input(f"ç‚¹æ•°ï¼ˆé˜¶æ®µ {s+1}ï¼Œå«ç«¯ç‚¹ï¼‰", 2, 50, 5, key=f"stage_n_{s}")
                        stage_cfg[s] = {"method": {"çº¿æ€§":"linear","äºŒæ¬¡":"quadratic","æŒ‡æ•°":"exponential","å¯¹æ•°":"logarithmic"}[mname],
                                        "num_points": int(pts)}
                st.session_state.stage_cfg = stage_cfg

                if st.button("ç”Ÿæˆæ’å€¼æ•°æ®ï¼ˆå¤šå˜é‡é˜¶æ®µå¼ï¼‰", use_container_width=True):
                    # æ„å»ºèŠ‚ç‚¹åˆ—è¡¨ï¼ˆå«èµ·ç‚¹ã€è¿‡æ¸¡èŠ‚ç‚¹ã€ç»ˆç‚¹ï¼‰å¹¶æ³¨å…¥ target
                    node_list = [ {**{f:p1[f] for f in selected_features}, target: p1[target]} ]
                    for i in range(num_nodes):
                        node = nodes[i].copy()
                        node[target] = st.session_state.node_targets[i]
                        node_list.append(node)
                    node_list.append( {**{f:p2[f] for f in selected_features}, target: p2[target]} )

                    # æŒ‰ node_list ç›¸é‚»ä¸¤ç‚¹è¿›è¡Œé˜¶æ®µå¼æ’å€¼
                    all_points = []
                    for s in range(len(node_list)-1):
                        a = node_list[s]
                        b = node_list[s+1]
                        cfg = st.session_state.stage_cfg[s]
                        pts = cfg["num_points"]
                        meth = cfg["method"]
                        seg = interpolate_between_points(
                            a, b, selected_features + [target], target,
                            num_points=pts, method=meth, feature_types=feature_types
                        )
                        # è¿æ¥æ—¶é¿å…é‡å¤ä¸­é—´èŠ‚ç‚¹ï¼ˆå»æ‰æ®µå†…ç¬¬ä¸€ä¸ªç‚¹ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªæ®µçš„ç¬¬ä¸€ä¸ªç‚¹ï¼‰
                        if s > 0 and len(seg) > 0:
                            seg = seg[1:]
                        all_points.extend(seg)

                    final_df = pd.DataFrame(all_points)
                    st.subheader("é˜¶æ®µå¼æ’å€¼ç»“æœ")
                    st.dataframe(final_df)

                    # å¤šå›¾å¯è§†åŒ–ï¼šæ¯ä¸ªæ•°å€¼ç‰¹å¾å¯¹ target çš„é˜¶æ®µç»“æœï¼ˆæŒ‰èŠ‚ç‚¹é¡ºåºï¼‰
                    st.subheader("å¯è§†åŒ–ï¼šå„æ•°å€¼ç‰¹å¾ä¸ç›®æ ‡çš„æ’å€¼è½¨è¿¹")
                    numeric_features = [f for f in selected_features if feature_types.get(f) == "æ•°å€¼å‹"]
                    for f in numeric_features:
                        x = final_df[f].tolist()
                        y = final_df[target].tolist()
                        visualize_xy(x, y, f, target, f"{f} â†’ {target}ï¼ˆé˜¶æ®µå¼ï¼‰",
                                     orig={'x':[p1[f], p2[f]], 'y':[p1[target], p2[target]]})

                    # ä¿å­˜åˆ°çŠ¶æ€ & ç´¯è®¡ç¼“å­˜
                    st.session_state.interpolated_data = final_df
                    st.session_state.interp_accum = pd.concat([st.session_state.interp_accum, final_df], ignore_index=True)
                    st.success(f"å·²è¿½åŠ åˆ°æ’å€¼ç¼“å­˜ï¼Œå½“å‰ç¼“å­˜å…± {len(st.session_state.interp_accum)} è¡Œã€‚")

            else:
                st.info("ä¸¤ç‚¹åœ¨æ‰€é€‰ç‰¹å¾ä¸Šå¹¶æ— å·®å¼‚ï¼Œæ— éœ€æ’å€¼ã€‚")

            # ---------- æ’å€¼ç¼“å­˜åŒº & å¯¼å‡º/è¿½åŠ  ----------
            st.subheader("ğŸ“¦ æ’å€¼ç¼“å­˜")
            if not st.session_state.interp_accum.empty:
                st.dataframe(st.session_state.interp_accum.tail(50))
                cexp1, cexp2, cexp3 = st.columns([1,1,1])

                with cexp1:
                    # æ–°æ–‡ä»¶å¯¼å‡º
                    fn = st.text_input("å¯¼å‡ºä¸ºï¼ˆæ–°æ–‡ä»¶åï¼‰", "interpolated_accum.csv")
                    if st.button("å¯¼å‡ºå½“å‰ç¼“å­˜ä¸ºæ–°CSV", use_container_width=True):
                        csv = st.session_state.interp_accum.to_csv(index=False).encode('utf-8')
                        st.download_button("ä¸‹è½½ CSV", csv, file_name=fn, mime="text/csv", use_container_width=True)

                with cexp2:
                    # è¿½åŠ åˆ°å·²æœ‰ CSVï¼ˆä¸Šä¼ å·²æœ‰æ–‡ä»¶ï¼Œè¿”å›åˆå¹¶åçš„ä¸‹è½½ï¼‰
                    append_file = st.file_uploader("é€‰æ‹©ä¸€ä¸ªå·²æœ‰CSVä»¥è¿½åŠ ", type=['csv'], key="append_csv")
                    if append_file is not None:
                        base = pd.read_csv(append_file)
                        merged = pd.concat([base, st.session_state.interp_accum], ignore_index=True)
                        st.write("é¢„è§ˆåˆå¹¶ç»“æœï¼ˆå°¾éƒ¨ï¼‰")
                        st.dataframe(merged.tail(50))
                        csvm = merged.to_csv(index=False).encode('utf-8')
                        st.download_button("ä¸‹è½½åˆå¹¶åçš„ CSV", csvm, file_name="merged_appended.csv", mime="text/csv", use_container_width=True)

                with cexp3:
                    if st.button("æ¸…ç©ºæ’å€¼ç¼“å­˜", use_container_width=True, type="secondary"):
                        st.session_state.interp_accum = pd.DataFrame()
                        st.success("ç¼“å­˜å·²æ¸…ç©ºã€‚")

        except Exception as e:
            st.error(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            st.info("è¯·å°è¯•å°†æ–‡ä»¶å¦å­˜ä¸º UTF-8 ç¼–ç åé‡æ–°ä¸Šä¼ ã€‚")

    else:
        st.info("è¯·å…ˆä¸Šä¼  5~10 ç»„å°æ ·æœ¬ CSV å¼€å§‹æ„é€ æ’å€¼æ•°æ®ã€‚")
    
# é¡µé¢1: æ•°æ®ä¸Šä¼ ä¸è®¾ç½®
elif page == "æ•°æ®ä¸Šä¼ ä¸è®¾ç½®":
    st.header("ğŸ“Š æ•°æ®ä¸Šä¼ ä¸ç‰¹å¾è®¾ç½®")
    
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ•°æ®æ–‡ä»¶", type=['csv'])
    
    if uploaded_file is not None:
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹å¹¶æ£€æµ‹ç¼–ç 
            file_content = uploaded_file.read()
            encoding = detect_encoding(file_content)
            
            # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            uploaded_file.seek(0)
            
            # ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç è¯»å–CSV
            df = pd.read_csv(uploaded_file, encoding=encoding)
            
            # å°è¯•å¤„ç†å¯èƒ½çš„ä¸­æ–‡åˆ—åé—®é¢˜
            if df.empty or df.columns.str.contains('Unnamed').any():
                # å¦‚æœåˆ—åæœ‰é—®é¢˜ï¼Œå°è¯•é‡æ–°è¯»å–
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, encoding=encoding, header=0)
                
            st.session_state.data = df
            
            # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
            st.subheader("æ•°æ®é¢„è§ˆ")
            st.dataframe(df.head())
            
            st.write(f"æ•°æ®é›†å½¢çŠ¶: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")
            st.write(f"æ£€æµ‹åˆ°çš„æ–‡ä»¶ç¼–ç : {encoding}")
            
            # é€‰æ‹©ç›®æ ‡å˜é‡
            target_options = df.columns.tolist()
            st.session_state.target = st.selectbox("é€‰æ‹©é¢„æµ‹ç›®æ ‡(è§‚æµ‹é‡)", target_options)
            
            # é€‰æ‹©ç‰¹å¾
            feature_options = [col for col in df.columns if col != st.session_state.target]
            selected_features = st.multiselect("é€‰æ‹©ç‰¹å¾å‚æ•°", feature_options, default=feature_options)
            st.session_state.features = selected_features
            
            # è®¾ç½®ç‰¹å¾ç±»å‹
            st.subheader("è®¾ç½®ç‰¹å¾ç±»å‹")
            st.write("è¯·ä¸ºæ¯ä¸ªç‰¹å¾æŒ‡å®šç±»å‹ï¼ˆæ•°å€¼å‹æˆ–åˆ†ç±»å‹ï¼‰")
            
            feature_types = {}
            cols = st.columns(2)
            for i, feature in enumerate(selected_features):
                with cols[i % 2]:
                    # å°è¯•è‡ªåŠ¨åˆ¤æ–­ç±»å‹
                    if df[feature].dtype in ['int64', 'float64']:
                        default_type = "æ•°å€¼å‹"
                    else:
                        default_type = "åˆ†ç±»å‹"
                    
                    feature_type = st.selectbox(
                        f"{feature} ç±»å‹", 
                        ["æ•°å€¼å‹", "åˆ†ç±»å‹"], 
                        index=0 if default_type == "æ•°å€¼å‹" else 1,
                        key=f"type_{feature}"
                    )
                    feature_types[feature] = feature_type
            
            st.session_state.feature_types = feature_types
            
            # æ•°æ®å¢å¼ºè®¾ç½®
            st.subheader("æ•°æ®å¢å¼ºè®¾ç½®")
            st.write("å°æ•°æ®é›†å»ºè®®ä½¿ç”¨æ•°æ®å¢å¼ºæé«˜æ¨¡å‹ç¨³å®šæ€§")
            
            augmentation_method = st.selectbox(
                "æ•°æ®å¢å¼ºæ–¹æ³•",
                ["æ— ", "é«˜æ–¯å™ªå£°", "è‡ªåŠ©æ³•", "æ’å€¼æ³•", "SMOTEå¼"],
                help="å°æ•°æ®é‡æ¨èï¼šé«˜æ–¯å™ªå£°ã€æ’å€¼æ³•ã€SMOTEå¼"
            )
            
            augmentation_factor = st.slider(
                "æ•°æ®å¢å¼ºå€æ•°", 
                1.0, 100.0, 1.5, 0.1,
                help="1.0è¡¨ç¤ºä¸å¢å¼ºï¼Œ2.0è¡¨ç¤ºæ•°æ®é‡ç¿»å€"
            )
            
            st.session_state.augmentation_method = augmentation_method
            st.session_state.augmentation_factor = augmentation_factor
            
            # åŸºç‰‡ç±»å‹é€‰æ‹©
            st.subheader("åŸºç‰‡ç±»å‹è®¾ç½®")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŸºç‰‡ç±»å‹çš„ç‰¹å¾
            substrate_features = [f for f in selected_features if "substrate" in f.lower() or "åŸºç‰‡" in f.lower()]
            
            if substrate_features:
                substrate_feature = substrate_features[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŸºç‰‡ç›¸å…³ç‰¹å¾
                substrate_options = df[substrate_feature].unique().tolist()
                
                st.session_state.substrate_option = st.radio(
                    "é€‰æ‹©åŸºç‰‡æ•°æ®å¤„ç†æ–¹å¼",
                    ["æ‰€æœ‰åŸºç‰‡", "ç‰¹å®šåŸºç‰‡"],
                    help="é€‰æ‹©æ˜¯å¦é’ˆå¯¹ç‰¹å®šåŸºç‰‡ç±»å‹è®­ç»ƒæ¨¡å‹"
                )
                
                if st.session_state.substrate_option == "ç‰¹å®šåŸºç‰‡":
                    st.session_state.selected_substrate = st.selectbox(
                        "é€‰æ‹©åŸºç‰‡ç±»å‹", 
                        substrate_options
                    )
                
                st.session_state.substrate_feature = substrate_feature
            else:
                st.info("æœªæ£€æµ‹åˆ°åŸºç‰‡ç±»å‹ç‰¹å¾ã€‚å¦‚æœæ‚¨æœ‰åŸºç‰‡ç±»å‹æ•°æ®ï¼Œè¯·ç¡®ä¿ç‰¹å¾åç§°ä¸­åŒ…å«'substrate'æˆ–'åŸºç‰‡'")
                st.session_state.substrate_option = "æ‰€æœ‰åŸºç‰‡"
                st.session_state.selected_substrate = None
            
            st.success("æ•°æ®è®¾ç½®å®Œæˆï¼è¯·è½¬åˆ°ã€Œæ¨¡å‹è®­ç»ƒã€é¡µé¢è®­ç»ƒæ¨¡å‹")
            
        except Exception as e:
            st.error(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            st.info("è¯·å°è¯•å°†æ–‡ä»¶å¦å­˜ä¸ºUTF-8ç¼–ç æ ¼å¼åé‡æ–°ä¸Šä¼ ")

# é¡µé¢2: æ¨¡å‹è®­ç»ƒ
elif page == "æ¨¡å‹è®­ç»ƒ":
    st.header("ğŸ¤– æ¨¡å‹è®­ç»ƒ")
    
    if st.session_state.data is None:
        st.warning("è¯·å…ˆåœ¨ã€Œæ•°æ®ä¸Šä¼ ä¸è®¾ç½®ã€é¡µé¢ä¸Šä¼ æ•°æ®å¹¶è®¾ç½®å‚æ•°")
    else:
        df = st.session_state.data
        target = st.session_state.target
        features = st.session_state.features
        feature_types = st.session_state.feature_types
        
        # æ ¹æ®åŸºç‰‡é€‰æ‹©è¿‡æ»¤æ•°æ®
        if st.session_state.substrate_option == "ç‰¹å®šåŸºç‰‡" and st.session_state.selected_substrate:
            substrate_feature = st.session_state.substrate_feature
            df = df[df[substrate_feature] == st.session_state.selected_substrate]
            st.write(f"ä½¿ç”¨åŸºç‰‡ç±»å‹: **{st.session_state.selected_substrate}**")
            st.write(f"ç­›é€‰åæ•°æ®é‡: {len(df)} æ¡")
        
        st.write(f"ç›®æ ‡å˜é‡: **{target}**")
        st.write(f"ç‰¹å¾å˜é‡: {', '.join(features)}")
        
        if len(df) < 10:
            st.error("æ•°æ®é‡è¿‡å°‘ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆçš„æ¨¡å‹è®­ç»ƒã€‚è¯·é€‰æ‹©å…¶ä»–åŸºç‰‡ç±»å‹æˆ–ä½¿ç”¨æ‰€æœ‰åŸºç‰‡æ•°æ®ã€‚")
        else:
            # åˆ’åˆ†æ•°æ®
            test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.2, 0.05)
            random_state = st.number_input("éšæœºç§å­", 0, 100, 42)
            
            X = df[features]
            y = df[target]
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state
            )
            
            # æ•°æ®å¢å¼º
            st.subheader("æ•°æ®å¢å¼º")
            augmentation_method = st.session_state.augmentation_method
            augmentation_factor = st.session_state.augmentation_factor
            
            if augmentation_method != "æ— ":
                st.write(f"ä½¿ç”¨æ•°æ®å¢å¼ºæ–¹æ³•: **{augmentation_method}**")
                st.write(f"å¢å¼ºå€æ•°: **{augmentation_factor}**")
                
                original_size = len(X_train)
                X_train, y_train = augment_data(
                    X_train, y_train, 
                    method=augmentation_method,
                    factor=augmentation_factor,
                    feature_types=feature_types
                )
                new_size = len(X_train)
                
                st.write(f"è®­ç»ƒæ•°æ®ä» {original_size} æ¡å¢å¼ºåˆ° {new_size} æ¡")
          
            # æ„å»ºé¢„å¤„ç†ç®¡é“
            numeric_features = [f for f in features if feature_types[f] == "æ•°å€¼å‹"]
            categorical_features = [f for f in features if feature_types[f] == "åˆ†ç±»å‹"]
            
            # å¦‚æœåŸºç‰‡ç‰¹å¾å·²ç»åœ¨åˆ†ç±»ç‰¹å¾ä¸­ï¼Œç¡®ä¿å®ƒä¸ä¼šè¢«é‡å¤å¤„ç†
            if hasattr(st.session_state, 'substrate_feature') and st.session_state.substrate_feature in categorical_features:
                categorical_features = [f for f in categorical_features if f != st.session_state.substrate_feature]
            
            preprocessor = ColumnTransformer([
                ('num', StandardScaler(), numeric_features),
                ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
            ])
            
            # æ¨¡å‹é€‰æ‹©
            st.subheader("æ¨¡å‹é€‰æ‹©ä¸å‚æ•°è®¾ç½®")
            model_type = st.selectbox(
                "é€‰æ‹©æœºå™¨å­¦ä¹ ç®—æ³•",
                ["éšæœºæ£®æ—å›å½’", "çº¿æ€§å›å½’", "Ridgeå›å½’", "Lassoå›å½’", "æ”¯æŒå‘é‡å›å½’(SVR)"],
                help="å°æ•°æ®é‡æ¨èï¼šçº¿æ€§å›å½’ã€Ridgeå›å½’ã€Lassoå›å½’\nå¤§æ•°æ®é‡æ¨èï¼šéšæœºæ£®æ—ã€SVR"
            )
            
            # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹ç±»å‹è®¾ç½®ç›¸åº”çš„è¶…å‚æ•°
            model_params = {}
            if model_type == "éšæœºæ£®æ—å›å½’":
                model_params['n_estimators'] = st.slider("æ ‘çš„æ•°é‡", 10, 500, 100, 10)
                model_params['max_depth'] = st.slider("æœ€å¤§æ·±åº¦", 1, 50, None, 1)
            elif model_type in ["Ridgeå›å½’", "Lassoå›å½’"]:
                model_params['alpha'] = st.slider("æ­£åˆ™åŒ–å¼ºåº¦", 0.01, 10.0, 1.0, 0.01)
                model_params['max_iter'] = st.slider("æœ€å¤§è¿­ä»£æ¬¡æ•°", 100, 10000, 1000, 100)
            elif model_type == "æ”¯æŒå‘é‡å›å½’(SVR)":
                model_params['kernel'] = st.selectbox("æ ¸å‡½æ•°", ["linear", "poly", "rbf", "sigmoid"], index=2)
                model_params['C'] = st.slider("æƒ©ç½šå‚æ•°C", 0.1, 100.0, 1.0, 0.1)
                model_params['gamma'] = st.selectbox("æ ¸å‡½æ•°ç³»æ•°gamma", ["scale", "auto"], index=0) 
            # è®­ç»ƒå®Œæˆåä¿å­˜å¢å¼ºæ•°æ®å’Œé¢„æµ‹ç»“æœåˆ°session_state
            st.session_state.X_train_augmented = X_train
            st.session_state.y_train_augmented = y_train
            st.session_state.X_test = X_test
            # è®­ç»ƒæ¨¡å‹æŒ‰é’®
            if st.button("è®­ç»ƒæ¨¡å‹"):
                with st.spinner("æ¨¡å‹è®­ç»ƒä¸­..."):
                    # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹ç±»å‹åˆ›å»ºç›¸åº”çš„å›å½’å™¨
                    if model_type == "éšæœºæ£®æ—å›å½’":
                        regressor = RandomForestRegressor(
                            n_estimators=model_params['n_estimators'],
                            max_depth=model_params['max_depth'],
                            random_state=random_state
                        )
                    elif model_type == "çº¿æ€§å›å½’":
                        from sklearn.linear_model import LinearRegression
                        regressor = LinearRegression()
                    elif model_type == "Ridgeå›å½’":
                        from sklearn.linear_model import Ridge
                        regressor = Ridge(
                            alpha=model_params['alpha'],
                            max_iter=model_params['max_iter'],
                            random_state=random_state
                        )
                    elif model_type == "Lassoå›å½’":
                        from sklearn.linear_model import Lasso
                        regressor = Lasso(
                            alpha=model_params['alpha'],
                            max_iter=model_params['max_iter'],
                            random_state=random_state
                        )
                    elif model_type == "æ”¯æŒå‘é‡å›å½’(SVR)":
                        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„å‚æ•°éƒ½å·²è®¾ç½®
                        if 'kernel' not in model_params:
                            model_params['kernel'] = 'rbf'  # è®¾ç½®é»˜è®¤å€¼
                        if 'C' not in model_params:
                            model_params['C'] = 1.0  # è®¾ç½®é»˜è®¤å€¼
                        if 'gamma' not in model_params:
                            model_params['gamma'] = 'scale'  # è®¾ç½®é»˜è®¤å€¼
                        
                        regressor = SVR(
                            kernel=model_params['kernel'],
                            C=model_params['C'],
                            gamma=model_params['gamma']
                        )
                    
                    # å»ºç«‹æ¨¡å‹ç®¡é“
                    model = Pipeline([
                        ('preprocessor', preprocessor),
                        ('regressor', regressor)
                    ])
                    
                    # è®­ç»ƒæ¨¡å‹
                    model.fit(X_train, y_train)
                    
                    # è¯„ä¼°æ¨¡å‹
                    # è¯„ä¼°æ¨¡å‹
                    y_pred = model.predict(X_test)
                    r2 = r2_score(y_test, y_pred)
                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                    
                    # ä¿å­˜æ¨¡å‹åˆ°session state
                    st.session_state.model = model
                    st.session_state.trained = True
                    st.session_state.model_type = model_type  # ä¿å­˜æ¨¡å‹ç±»å‹ç”¨äºåç»­åˆ†æ
                    st.session_state.y_pred = y_pred
                    # æ˜¾ç¤ºç»“æœ
                    st.success(f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼ä½¿ç”¨ç®—æ³•: {model_type}")
                    st.metric("RÂ² åˆ†æ•°", f"{r2:.4f}")
                    st.metric("RMSE", f"{rmse:.4f}")
                    
                    # çœŸå®å€¼ vs é¢„æµ‹å€¼å›¾
                    fig, ax = plt.subplots()
                    ax.scatter(y_test, y_pred, alpha=0.5)
                    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
                    ax.set_xlabel('çœŸå®å€¼')
                    ax.set_ylabel('é¢„æµ‹å€¼')
                    ax.set_title('çœŸå®å€¼ vs é¢„æµ‹å€¼')
                    st.pyplot(fig)
                    
                    # æ”¹è¿›çš„æ¨¡å‹ä¿å­˜åŠŸèƒ½
                    if st.session_state.trained:
                        st.subheader("ğŸ’¾ æ¨¡å‹ä¿å­˜")
                        
                        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
                        col1, col2 = st.columns([2, 1])
                        
                        with col1:
                            # æä¾›é»˜è®¤æ–‡ä»¶åå»ºè®®
                            default_filename = f"{st.session_state.model_type}_{st.session_state.target}_model.joblib"
                            model_filename = st.text_input("æ¨¡å‹ä¿å­˜æ–‡ä»¶å", value=default_filename)
                        
                        with col2:
                            st.markdown("<br>", unsafe_allow_html=True)  # å‚ç›´å¯¹é½
                            # æ·»åŠ æ–‡ä»¶æ ¼å¼è¯´æ˜
                            st.caption("æ–‡ä»¶æ ¼å¼: .joblib")
                        
                        # æ·»åŠ æ¨¡å‹ä¿¡æ¯æ‘˜è¦
                        st.info(f"""
                        **æ¨¡å‹ä¿¡æ¯æ‘˜è¦:**
                        - æ¨¡å‹ç±»å‹: {st.session_state.model_type}
                        - é¢„æµ‹ç›®æ ‡: {st.session_state.target}
                        - ç‰¹å¾æ•°é‡: {len(st.session_state.features)}
                        - è®­ç»ƒæ•°æ®é‡: {len(st.session_state.X_train_augmented) if hasattr(st.session_state, 'X_train_augmented') else 'æœªçŸ¥'}
                        """)
                        
                        # å°†æ¨¡å‹è½¬æ¢ä¸ºå­—èŠ‚æµ
                        try:
                            model_bytes = io.BytesIO()
                            joblib.dump({"model": st.session_state.model, "target": st.session_state.target}, model_bytes)
                            model_bytes.seek(0)
                            
                            # æ·»åŠ æ–‡ä»¶å¤§å°ä¿¡æ¯
                            file_size = len(model_bytes.getvalue())
                            size_mb = file_size / (1024 * 1024)
                            st.caption(f"é¢„è®¡æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")
                            
                            # æä¾›ä¸‹è½½æŒ‰é’®
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½è®­ç»ƒå¥½çš„æ¨¡å‹",
                                data=model_bytes,
                                file_name=model_filename,
                                mime="application/octet-stream",
                                help="ç‚¹å‡»ä¸‹è½½è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ï¼Œå¯ç”¨äºåç»­é¢„æµ‹"
                            )
                            
                            # æ·»åŠ æ¨¡å‹ä¿å­˜æç¤º
                            st.success("æ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä¸‹è½½ä¿å­˜ã€‚å»ºè®®å°†æ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨å®‰å…¨çš„ä½ç½®ã€‚")
                            
                        except Exception as e:
                            st.error(f"æ¨¡å‹ä¿å­˜è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                            
                        # æ·»åŠ æ¨¡å‹ä½¿ç”¨è¯´æ˜
                        with st.expander("ğŸ“– æ¨¡å‹ä½¿ç”¨è¯´æ˜"):
                            st.markdown("""
                            ### å¦‚ä½•ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹
                            
                            1. **åŠ è½½æ¨¡å‹**:
                            ```python
                            import joblib
                            model = joblib.load('your_model_filename.joblib')
                            ```
                            
                            2. **è¿›è¡Œé¢„æµ‹**:
                            ```python
                            # å‡†å¤‡è¾“å…¥æ•°æ® (ä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾é¡ºåº)
                            input_data = [[value1, value2, ...]]
                            prediction = model.predict(input_data)
                            ```
                            
                            3. **æ³¨æ„äº‹é¡¹**:
                            - ç¡®ä¿ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ•°æ®é¢„å¤„ç†æ­¥éª¤
                            - è¾“å…¥æ•°æ®çš„ç‰¹å¾é¡ºåºå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´
                            - åˆ†ç±»ç‰¹å¾éœ€è¦ä½¿ç”¨ç›¸åŒçš„ç¼–ç æ–¹å¼
                            
                            ### æ¨¡å‹æ–‡ä»¶å†…å®¹
                            - è®­ç»ƒå¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹
                            - æ•°æ®é¢„å¤„ç†ç®¡é“ï¼ˆæ ‡å‡†åŒ–ã€ç¼–ç ç­‰ï¼‰
                            - ç‰¹å¾åç§°å’Œç±»å‹ä¿¡æ¯
                            """)
                    else:
                        st.warning("è¯·å…ˆè®­ç»ƒæ¨¡å‹ï¼Œç„¶åæ‰èƒ½ä¿å­˜ã€‚")

# é¡µé¢3: å‚æ•°é¢„æµ‹
elif page == "å‚æ•°é¢„æµ‹":
    st.header("ğŸ”® å‚æ•°é¢„æµ‹")

    # ========== æ–°å¢ï¼šä¸Šä¼ è®­ç»ƒå¥½çš„å•æ¨¡å‹ï¼ˆjoblib/pklï¼‰ ==========
    use_uploaded_model = False
    uploaded_model = st.file_uploader("ä¸Šä¼ è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆjoblibæ ¼å¼ï¼‰", type=['pkl', 'joblib'], key="pred_model_upload")
    if uploaded_model is not None:
        model_loaded, target_loaded, features_loaded, ftypes_loaded = _load_model_from_joblib(uploaded_model)
        # è‹¥å¯¹è±¡é‡Œæºå¸¦ targetï¼Œæ›´æ–°ä¼šè¯ target
        if target_loaded is not None:
            st.session_state.target = target_loaded
        # è‹¥æºå¸¦ features/feature_typesï¼Œåˆ™ä¹Ÿå­˜èµ·æ¥ä¾›æœ¬é¡µæ§ä»¶ä½¿ç”¨
        if features_loaded:
            st.session_state.features = features_loaded
        if ftypes_loaded:
            st.session_state.feature_types = ftypes_loaded

        st.success(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œç›®æ ‡å˜é‡ï¼š{st.session_state.get('target', 'æœªçŸ¥')}")
        st.session_state.model = model_loaded
        use_uploaded_model = True

    # ========== å…¼å®¹ï¼šè‹¥æ²¡ä¸Šä¼ ï¼Œåˆ™ä½¿ç”¨ä¼šè¯å†…è®­ç»ƒå¥½çš„æ¨¡å‹ ==========
    if not use_uploaded_model:
        if not st.session_state.trained or st.session_state.model is None:
            st.warning("è¯·å…ˆä¸Šä¼ æ¨¡å‹ï¼Œæˆ–åœ¨ã€Œæ¨¡å‹è®­ç»ƒã€é¡µé¢è®­ç»ƒæ¨¡å‹åå†æ¥é¢„æµ‹ã€‚")
            st.stop()

    model = st.session_state.model
    features = st.session_state.features
    feature_types = st.session_state.feature_types

    st.subheader("è¾“å…¥å‚æ•°è¿›è¡Œé¢„æµ‹")
    input_data = {}
    cols = st.columns(2)

    # è‹¥æœ‰åŸºç‰‡ç­‰åˆ†ç±»ç‰¹å¾ï¼Œå¯ä»¥åƒä½ åŸæ¥ä¸€æ ·å¤„ç†ï¼›ä¸‹é¢æ˜¯é€šç”¨å¤„ç†ï¼š
    for i, feature in enumerate(features):
        with cols[i % 2]:
            if feature_types.get(feature) == "æ•°å€¼å‹":
                data_min = float(st.session_state.data[feature].min())
                data_max = float(st.session_state.data[feature].max())
                input_data[feature] = st.slider(
                    f"{feature}",
                    min_value=data_min, max_value=data_max,
                    value=(data_min + data_max) / 2, step=1.0,
                    key=f"input_{feature}"
                )
            else:
                options = st.session_state.data[feature].unique().tolist()
                input_data[feature] = st.selectbox(
                    f"{feature}", options, key=f"input_{feature}"
                )

    input_df = pd.DataFrame([input_data])

    if st.button("é¢„æµ‹"):
        pred = model.predict(input_df)
        pred_val = float(np.ravel(pred)[0])
        st.success(f"ğŸ”¬ é¢„æµ‹çš„ **{st.session_state.target}** ä¸ºï¼š{pred_val:.4f}")
        st.subheader("è¾“å…¥å‚æ•°è¯¦æƒ…")
        st.dataframe(input_df)
        
        # ä¿å­˜é¢„æµ‹ç»“æœåˆ°session state
        st.session_state.last_prediction = pred_val
        st.session_state.last_input_data = input_data
    
    # ... (ä¿æŒåŸæœ‰ä»£ç ä¸å˜)
        # æ–°å¢éƒ¨åˆ†ï¼šCrSbè–„è†œç”Ÿé•¿ç»“æŸå›¾åƒå±•ç¤ºï¼ˆåŸºäºå®é™…ç»“æ„çš„ç¤ºæ„å›¾ï¼‰
        st.markdown("---")
        st.subheader("CrSbè–„è†œåŸå­ç»“æ„ç¤ºæ„å›¾")
        
        # ä½¿ç”¨ä¸¤åˆ—å¸ƒå±€
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**CrSbæ™¶ä½“ç»“æ„ç¤ºæ„å›¾**")
            
            # åˆ›å»ºCrSbæ™¶ä½“ç»“æ„ç¤ºæ„å›¾
            fig, ax = plt.subplots(figsize=(6, 6))
            
            # CrSbå…·æœ‰NiAså‹ç»“æ„ï¼ˆå…­æ–¹æ™¶ç³»ï¼‰
            # æ™¶æ ¼å¸¸æ•°: a = 4.12 Ã…, c = 5.42 Ã…
            a = 1.0  # ç›¸å¯¹æ™¶æ ¼å¸¸æ•°a
            c = 1.3  # ç›¸å¯¹æ™¶æ ¼å¸¸æ•°c (c/a â‰ˆ 1.315ï¼Œç¬¦åˆCrSbçš„å®é™…æ¯”ä¾‹)
            
            # è·å–åŒ–å­¦è®¡é‡æ¯”ï¼ˆä»é¢„æµ‹æ•°æ®æˆ–è¾“å…¥æ•°æ®ï¼‰
            stoichiometry = 1.0  # é»˜è®¤1:1
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŒ–å­¦è®¡é‡æ¯”ç›¸å…³çš„å‚æ•°
            stoichiometry_params = [f for f in features if "ratio" in f.lower() or "åŒ–å­¦è®¡é‡" in f or "stoichiometry" in f.lower()]
            
            # å¦‚æœå·²è¿›è¡Œé¢„æµ‹ï¼Œå°è¯•ä»é¢„æµ‹æ•°æ®è·å–åŒ–å­¦è®¡é‡æ¯”
            if hasattr(st.session_state, 'last_input_data'):
                # å¦‚æœç›®æ ‡å˜é‡æ˜¯åŒ–å­¦è®¡é‡æ¯”ï¼Œä½¿ç”¨é¢„æµ‹å€¼
                if any(keyword in st.session_state.target.lower() for keyword in ["ratio", "åŒ–å­¦è®¡é‡", "stoichiometry"]):
                    stoichiometry = st.session_state.last_prediction
                # å¦åˆ™ä»è¾“å…¥ç‰¹å¾ä¸­å¯»æ‰¾åŒ–å­¦è®¡é‡æ¯”å‚æ•°
                elif stoichiometry_params:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„åŒ–å­¦è®¡é‡æ¯”å‚æ•°
                    stoichiometry_param = stoichiometry_params[0]
                    stoichiometry = st.session_state.last_input_data.get(stoichiometry_param, 1.0)
            
            # ç¡®ä¿stoichiometryä¸ä¸ºé›¶ï¼Œé¿å…é™¤ä»¥é›¶é”™è¯¯
            if stoichiometry <= 0:
                stoichiometry = 1.0
                
            # è®¡ç®—Crå’ŒSbçš„æ¯”ä¾‹
            cr_ratio = 1.0 / (1.0 + stoichiometry) if stoichiometry != 1.0 else 0.5
            sb_ratio = stoichiometry / (1.0 + stoichiometry) if stoichiometry != 1.0 else 0.5
            
            # ç»˜åˆ¶å…­æ–¹æ™¶æ ¼ç»“æ„ - æ›´ç¬¦åˆå®é™…çš„CrSbç»“æ„
            # åœ¨NiAså‹ç»“æ„ä¸­ï¼ŒCrå’ŒSbåŸå­äº¤æ›¿æ’åˆ—å½¢æˆå…­æ–¹æœ€å¯†å †ç§¯
            
            # ç»˜åˆ¶åŸºç‰‡åº•å±‚
            ax.add_patch(plt.Rectangle((-0.5, -0.5), 4, 0.2, color='lightgray', alpha=0.7, label='åŸºç‰‡'))
            
            # ç»˜åˆ¶å¤šä¸ªæ™¶èƒä»¥æ˜¾ç¤ºå®Œæ•´çš„æ™¶ä½“ç»“æ„
            for i in range(3):
                for j in range(3):
                    # è®¡ç®—æ™¶èƒåŸç‚¹
                    origin_x = i * a
                    origin_y = j * c
                    
                    # ç»˜åˆ¶CråŸå­ï¼ˆçº¢è‰²ï¼‰- ä½äºå…­æ–¹æ™¶æ ¼çš„é¡¶ç‚¹å’Œä½“å¿ƒ
                    ax.scatter(origin_x, origin_y, color='red', s=100, label='Cr' if i == 0 and j == 0 else "")
                    ax.scatter(origin_x + a/2, origin_y + c/2, color='red', s=100)
                    
                    # ç»˜åˆ¶SbåŸå­ï¼ˆè“è‰²ï¼‰- ä½äºå…­æ–¹æ™¶æ ¼çš„é¢å¿ƒä½ç½®
                    ax.scatter(origin_x + a/2, origin_y, color='blue', s=100, label='Sb' if i == 0 and j == 0 else "")
                    ax.scatter(origin_x, origin_y + c/2, color='blue', s=100)
                    
                    # æ·»åŠ æ™¶èƒè¾¹ç•Œ
                    if i < 2 and j < 2:  # åªç»˜åˆ¶å†…éƒ¨æ™¶èƒè¾¹ç•Œ
                        ax.plot([origin_x, origin_x + a], [origin_y, origin_y], 'k-', alpha=0.3)
                        ax.plot([origin_x, origin_x], [origin_y, origin_y + c], 'k-', alpha=0.3)
                        ax.plot([origin_x + a, origin_x + a], [origin_y, origin_y + c], 'k-', alpha=0.3)
                        ax.plot([origin_x, origin_x + a], [origin_y + c, origin_y + c], 'k-', alpha=0.3)
            
            # æ·»åŠ æ™¶æ ¼æ–¹å‘æŒ‡ç¤º
            ax.arrow(0, 0, a, 0, head_width=0.1, head_length=0.1, fc='k', ec='k')
            ax.text(a/2, -0.2, '[100]', ha='center')
            
            ax.arrow(0, 0, 0, c, head_width=0.1, head_length=0.1, fc='k', ec='k')
            ax.text(-0.2, c/2, '[001]', ha='center', va='center')
            
            ax.set_xlim(-0.5, 3.0)
            ax.set_ylim(-0.5, 3.5)
            ax.set_xlabel('æ™¶æ ¼æ–¹å‘ [100]')
            ax.set_ylabel('æ™¶æ ¼æ–¹å‘ [001]')
            ax.set_title(f'CrSbå…­æ–¹æ™¶ä½“ç»“æ„ (NiAså‹)')
            ax.grid(False)
            ax.legend()
            
            st.pyplot(fig)
            
            st.markdown(f"""
            **æ™¶ä½“ç»“æ„ç‰¹å¾:**
            - ç©ºé—´ç¾¤: P6â‚ƒ/mmc (No. 194)
            - æ™¶æ ¼å¸¸æ•°: a = 4.12 Ã…, c = 5.42 Ã…
            - ç»“æ„ç±»å‹: NiAså‹å…­æ–¹ç»“æ„
            - åŸå­æ’åˆ—: Cr(çº¢è‰²)å’ŒSb(è“è‰²)åŸå­äº¤æ›¿æ’åˆ—å½¢æˆå…­æ–¹æœ€å¯†å †ç§¯
            - åŒ–å­¦è®¡é‡æ¯”: Cr:Sb â‰ˆ {1/stoichiometry:.2f}:{stoichiometry:.2f} (é¢„æµ‹å€¼)
            """)
       
            
        with col2:
            st.markdown("**CrSbè–„è†œç”Ÿé•¿ç¤ºæ„å›¾**")
        
            # åˆ›å»ºè–„è†œç”Ÿé•¿ç¤ºæ„å›¾
            fig, ax = plt.subplots(figsize=(6, 6))
            
            # ç»˜åˆ¶åŸºç‰‡
            ax.add_patch(plt.Rectangle((-0.25, 0), 4, 0.5, color='gray', alpha=0.7, label='åŸºç‰‡'))
            
            # è·å–åŒ–å­¦è®¡é‡æ¯”ï¼ˆå¦‚æœæœ‰ï¼‰
            stoichiometry = pred_val if 'prediction' in locals() else 1.0  # ä½¿ç”¨é¢„æµ‹å€¼æˆ–é»˜è®¤1:1
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŒ–å­¦è®¡é‡æ¯”ç›¸å…³çš„å‚æ•°
            if stoichiometry_params and hasattr(st.session_state, 'last_input_data'):
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„åŒ–å­¦è®¡é‡æ¯”å‚æ•°
                stoichiometry_param = stoichiometry_params[0]
                stoichiometry = st.session_state.last_input_data.get(stoichiometry_param, 1.0)
            
            # è®¡ç®—Crå’ŒSbçš„æ¯”ä¾‹
            cr_ratio = 1.0 / (1.0 + stoichiometry) if stoichiometry != 1.0 else 0.5
            sb_ratio = stoichiometry / (1.0 + stoichiometry) if stoichiometry != 1.0 else 0.5
            
           # ç»˜åˆ¶è–„è†œå±‚ï¼ˆå¤šå±‚åŸå­ï¼‰- éšæœºåˆ†å¸ƒç¼ºå¤±åŸå­
        # ç»˜åˆ¶è–„è†œå±‚ï¼ˆå¤šå±‚åŸå­ï¼‰- äº¤æ›¿æ”¾ç½® + ç¼ºå¤±éšæœºåˆ†å¸ƒ
            layers = 5
            atoms_per_layer = 6
            total_atoms = layers * atoms_per_layer

            # è®¡ç®—æ¯ç§åŸå­çš„æ€»æ•°
            cr_total_needed = (total_atoms + 1) // 2  # å¶æ•°æ ¼å­ä½ç½®
            sb_total_needed = total_atoms // 2        # å¥‡æ•°æ ¼å­ä½ç½®

            # æŒ‰æ¯”ä¾‹å†³å®šå®é™…èƒ½æ”¾çš„æ•°é‡
            cr_count = min(cr_total_needed, int(total_atoms * cr_ratio))
            sb_count = min(sb_total_needed, int(total_atoms * sb_ratio))

            # ç¡®å®šå“ªäº›ä½ç½®æ˜¯ Cr ä½ã€å“ªäº›æ˜¯ Sb ä½
            cr_positions = [i for i in range(total_atoms) if i % 2 == 0]
            sb_positions = [i for i in range(total_atoms) if i % 2 == 1]

            # éšæœºæŒ‘é€‰ç¼ºå¤±ä½ç½®
            missing_cr = set(np.random.choice(cr_positions, cr_total_needed - cr_count, replace=False)) \
                if cr_total_needed > cr_count else set()
            missing_sb = set(np.random.choice(sb_positions, sb_total_needed - sb_count, replace=False)) \
                if sb_total_needed > sb_count else set()

            atom_index = 0
            for layer in range(layers):
                y_pos = 0.5 + layer * 0.4
                for i in range(atoms_per_layer):
                    x_pos = i * 0.7

                    if atom_index % 2 == 0:  # Cr ä½ç½®
                        if atom_index in missing_cr:
                            ax.scatter(x_pos, y_pos, facecolors='none', edgecolors='black', s=80,
                                    linewidths=1.5, label='Crç¼ºå¤±' if atom_index == list(missing_cr)[0] else "")
                        else:
                            ax.scatter(x_pos, y_pos, color='red', s=80,
                                    label='CråŸå­' if atom_index == 0 else "")
                    else:  # Sb ä½ç½®
                        if atom_index in missing_sb:
                            ax.scatter(x_pos, y_pos, facecolors='none', edgecolors='black', s=80,
                                    linewidths=1.5, label='Sbç¼ºå¤±' if atom_index == list(missing_sb)[0] else "")
                        else:
                            ax.scatter(x_pos, y_pos, color='blue', s=80,
                                    label='SbåŸå­' if atom_index == 1 else "")

                    atom_index += 1


            ax.set_xlim(-0.5, 4.5)
            ax.set_ylim(0, 3.5)
            ax.set_xlabel('æ¨ªå‘ä½ç½®')
            ax.set_ylabel('ç”Ÿé•¿æ–¹å‘')
            ax.set_title(f'CrSbè–„è†œç”Ÿé•¿ç¤ºæ„å›¾ (Cr:Sb â‰ˆ {1}:{stoichiometry:.2f})')
            ax.legend()
            
            st.pyplot(fig)
            
            # æ ¹æ®åŒ–å­¦è®¡é‡æ¯”è¯„ä¼°è–„è†œè´¨é‡
            if abs(stoichiometry - 1.0) < 0.1:
                quality = "é«˜è´¨é‡"
                quality_desc = "æ¥è¿‘ç†æƒ³åŒ–å­¦è®¡é‡æ¯”ï¼Œæ™¶ä½“ç»“æ„å®Œæ•´ï¼Œç¼ºé™·å°‘"
            elif abs(stoichiometry - 1.0) < 0.3:
                quality = "ä¸­ç­‰è´¨é‡"
                quality_desc = "åŒ–å­¦è®¡é‡æ¯”ç•¥æœ‰åç¦»ï¼Œå¯èƒ½å­˜åœ¨å°‘é‡ç¼ºé™·"
            else:
                quality = "ä½è´¨é‡"
                quality_desc = "åŒ–å­¦è®¡é‡æ¯”ä¸¥é‡åç¦»ï¼Œå¯èƒ½å­˜åœ¨å¤§é‡ç¼ºé™·å’Œéæ™¶åŒºåŸŸ"
            
            st.markdown(f"""
            **ç”Ÿé•¿ç‰¹å¾:**
            - åŒ–å­¦è®¡é‡æ¯”: Cr:Sb â‰ˆ {1}:{stoichiometry:.2f} (é¢„æµ‹å€¼)
            - è–„è†œè´¨é‡: {quality}
            - è´¨é‡æè¿°: {quality_desc}
            - ç”Ÿé•¿æ¨¡å¼: å±‚çŠ¶å¤–å»¶ç”Ÿé•¿
            - åŸå­åˆ†å¸ƒ: {cr_count}ä¸ªCråŸå­, {sb_count}ä¸ªSbåŸå­
            """)
        # æ·»åŠ CrSbè–„è†œç‰¹æ€§è¯´æ˜
        st.markdown("""
        ### CrSbè–„è†œç‰¹æ€§è¯´æ˜
        
        CrSbæ˜¯ä¸€ç§é‡è¦çš„åŠé‡‘å±ç£æ€§ææ–™ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§:
        
        - **æ™¶ä½“ç»“æ„**: å…­æ–¹æ™¶ç³»ï¼ŒNiAså‹ç»“æ„ï¼Œç©ºé—´ç¾¤P6â‚ƒ/mmc
        - **ç£æ€§**: å®¤æ¸©é“ç£æ€§ï¼Œå±…é‡Œæ¸©åº¦çº¦700K
        - **ç”µå­ç»“æ„**: åŠé‡‘å±æ€§ï¼Œè‡ªæ—‹æåŒ–ç‡æ¥è¿‘100%
        - **åº”ç”¨**: è‡ªæ—‹ç”µå­å™¨ä»¶ã€ç£å­˜å‚¨è®¾å¤‡ã€ç£ä¼ æ„Ÿå™¨
        
        **åŒ–å­¦è®¡é‡æ¯”å¯¹è–„è†œè´¨é‡çš„å½±å“:**
        - ç†æƒ³åŒ–å­¦è®¡é‡æ¯” (Cr:Sb = 1:1): é«˜è´¨é‡æ™¶ä½“ï¼Œç£æ€§èƒ½æœ€ä½³
        - Crè¿‡é‡ (Cr:Sb > 1:1): å¯èƒ½å½¢æˆCrå›¢ç°‡ï¼Œé™ä½ç£æ€§èƒ½
        - Sbè¿‡é‡ (Cr:Sb < 1:1): å¯èƒ½å¯¼è‡´éæ™¶åŒºåŸŸï¼Œé™ä½ç»“æ™¶è´¨é‡
        
        **é«˜è´¨é‡CrSbè–„è†œçš„ç”Ÿé•¿å…³é”®å‚æ•°:**
        - åŸºç‰‡æ¸©åº¦: 250-400Â°C
        - æ²‰ç§¯é€Ÿç‡: 0.1-0.5 Ã…/s
        - æº…å°„åŠŸç‡: 50-100W (DCæº…å°„)
        - æ°©æ°”å‹åŠ›: 2-5 mTorr
        - åŸºç‰‡ç±»å‹: MgOã€Alâ‚‚Oâ‚ƒæˆ–Si with buffer layer
        """)
# é¡µé¢3-2ï¼šåå‘é¢„æµ‹ï¼ˆå¤šç›®æ ‡ï¼‰
elif page == "åå‘é¢„æµ‹":
    st.header("ğŸ”„ åå‘å‚æ•°æ¨èï¼ˆå¤šç›®æ ‡ä¼˜åŒ–ï¼‰")

    df_data = st.session_state.get("data", None)
    if df_data is None or df_data.empty:
        st.warning("è¯·å…ˆåœ¨å‰é¢çš„é¡µé¢ä¸Šä¼ æ•°æ®é›†ï¼Œå¹¶å®Œæˆç‰¹å¾/ç›®æ ‡è®¾ç½®ã€‚")
        st.stop()

    # ====== å¤šæ¨¡å‹ä¸Šä¼ ï¼šä¸€ä¸ª target å¯¹åº”ä¸€ä¸ªæ¨¡å‹ï¼ˆå¯å¤šé€‰ä¸Šä¼ ï¼‰======
    st.subheader("ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªå·²è®­ç»ƒæ¨¡å‹ï¼ˆæ¯ä¸ªæ–‡ä»¶ä¸€ä¸ªç›®æ ‡å˜é‡ï¼‰")
    uploaded_models = st.file_uploader(
        "æ”¯æŒ joblib/pklï¼Œå¯ä¸€æ¬¡é€‰æ‹©å¤šä¸ªæ–‡ä»¶",
        type=["pkl", "joblib"],
        accept_multiple_files=True,
        key="rev_models_upload"
    )

    models_by_target = {}   # {target: {"model":..., "features":..., "feature_types":...}}

    if uploaded_models:
        for f in uploaded_models:
            m, t, feats, ftypes = _load_model_from_joblib(f)
            # è‹¥ç¼ºå¤± targetï¼Œåˆ™å…è®¸ç”¨æˆ·è¡¥å……å‘½å
            if t is None:
                t = st.text_input(f"ä¸ºæ–‡ä»¶ {f.name} æŒ‡å®šè¯¥æ¨¡å‹çš„ç›®æ ‡å˜é‡å", value=f"target_{len(models_by_target)+1}", key=f"tname_{f.name}")
            st.write(f"âœ”ï¸ æ¨¡å‹ {f.name} åŠ è½½å®Œæˆï¼Œç›®æ ‡å˜é‡ï¼š**{t}**")
            models_by_target[t] = {"model": m, "features": feats, "feature_types": ftypes}

    # å…œåº•ï¼šè‹¥æœªä¸Šä¼ ä»»ä½•æ¨¡å‹ï¼Œåˆ™å°è¯•ä½¿ç”¨ä¼šè¯ä¸­è®­ç»ƒå¥½çš„å•æ¨¡å‹
    if not models_by_target:
        if st.session_state.get("trained", False) and st.session_state.get("model", None) is not None:
            t = st.session_state.get("target", "target")
            models_by_target[t] = {
                "model": st.session_state.model,
                "features": st.session_state.get("features", []),
                "feature_types": st.session_state.get("feature_types", {})
            }
            st.info(f"æœªä¸Šä¼ å¤–éƒ¨æ¨¡å‹ï¼Œä½¿ç”¨å½“å‰ä¼šè¯æ¨¡å‹ï¼ˆç›®æ ‡å˜é‡ï¼š{t}ï¼‰ã€‚")
        else:
            st.warning("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæ¨¡å‹ï¼Œæˆ–å…ˆåœ¨ä¼šè¯ä¸­è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚")
            st.stop()

    # ====== é€‰æ‹©è¦åŒæ—¶ä¼˜åŒ–çš„ç›®æ ‡ ======
    all_targets_loaded = list(models_by_target.keys())
    target_options = st.multiselect(
        "é€‰æ‹©éœ€è¦åŒæ—¶ä¼˜åŒ–çš„è–„è†œç‰¹æ€§ï¼ˆç›®æ ‡å˜é‡ï¼‰",
        options=all_targets_loaded,
        default=all_targets_loaded
    )
    if len(target_options) == 0:
        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç›®æ ‡ç‰¹æ€§")
        st.stop()

    # ====== ç»Ÿä¸€ç‰¹å¾&ç±»å‹ï¼ˆç”¨äºæ„é€ å€™é€‰æ ·æœ¬ï¼‰ã€‚è‹¥ä¸ç»Ÿä¸€ï¼Œåˆ™æŒ‰å€™é€‰é‡Œâ€œå¹¶é›†â€å¤„ç† ======
    # è‹¥ä¼šè¯é‡Œå·²æœ‰ features/feature_typesï¼Œä¼˜å…ˆä½¿ç”¨ï¼›å¦åˆ™æ ¹æ®æ¨¡å‹/æ•°æ®æ¨æ–­
    global_features = st.session_state.get("features", [])
    global_ftypes = st.session_state.get("feature_types", {})
    if not global_features:
        # ä»ä»»æ„ä¸€ä¸ªæ¨¡å‹ä¸­æ¨æ–­
        for t in target_options:
            global_features = models_by_target[t].get("features", [])
            if not global_features:
                global_features = _infer_features_if_missing(models_by_target[t]["model"], df_data, known_targets=target_options)
            if global_features:
                break
    # è‹¥è¿˜ä¸ºç©ºï¼Œå°±ç”¨æ•°æ®åˆ—å‡å»ç›®æ ‡åˆ—
    if not global_features:
        global_features = [c for c in df_data.columns if c not in target_options]
    if not global_ftypes:
        global_ftypes = _infer_feature_types_if_missing(global_features, df_data, given=None)

    # ====== å‚æ•°èŒƒå›´/å¯é€‰å€¼é™åˆ¶ï¼ˆæ•°å€¼ï¼šèŒƒå›´ï¼›åˆ†ç±»å‹ï¼šå¤šé€‰ï¼‰======
    st.markdown("### é™å®šç”Ÿé•¿å‚æ•°èŒƒå›´æˆ–å›ºå®šå€¼")
    param_limits = {}
    cols_lim = st.columns(2)
    for i, f in enumerate(global_features):
        with cols_lim[i % 2]:
            if global_ftypes.get(f) == "æ•°å€¼å‹":
                # ç»™å‡ºç¨æ‹“çš„èŒƒå›´ï¼ˆÂ±10%ï¼‰
                dmin = float(df_data[f].min())
                dmax = float(df_data[f].max())
                lo, hi = 0.9 * dmin, 1.1 * dmax
                # æ»‘æ¡èŒƒå›´
                slider_vals = st.slider(
                    f"{f} èŒƒå›´",
                    min_value=float(min(lo, dmin)), max_value=float(max(hi, dmax)),
                    value=(float(dmin), float(dmax)),
                    step=1.0, key=f"rev_rng_{f}"
                )
                # æ•°å­—è¾“å…¥ç²¾ä¿®
                c1, c2 = st.columns(2)
                with c1:
                    low_val = st.number_input(f"{f} æœ€å°å€¼", value=float(slider_vals[0]), step=1.0, key=f"rev_min_{f}")
                with c2:
                    high_val = st.number_input(f"{f} æœ€å¤§å€¼", value=float(slider_vals[1]), step=1.0, key=f"rev_max_{f}")
                if low_val > high_val:
                    low_val, high_val = high_val, low_val
                    st.warning(f"å·²è‡ªåŠ¨è°ƒæ•´ {f} çš„èŒƒå›´ä»¥ä¿è¯æœ€å°å€¼ â‰¤ æœ€å¤§å€¼")
                param_limits[f] = (float(low_val), float(high_val))
            else:
                opts = df_data[f].dropna().unique().tolist()
                selected = st.multiselect(f"{f} å¯é€‰å€¼", options=opts, default=opts, key=f"rev_cat_{f}")
                if len(selected) == 0:
                    st.warning(f"{f} å½“å‰æ— å¯é€‰å€¼ï¼Œå·²ä¸´æ—¶ä½¿ç”¨å…¨éƒ¨å†å²å€¼ä½œä¸ºå€™é€‰ã€‚")
                    selected = opts
                param_limits[f] = selected

    # ====== è®¾ç½®ç›®æ ‡å€¼å’Œæƒé‡ ======
    st.markdown("### è®¾ç½®ç›®æ ‡è–„è†œç‰¹æ€§æœŸæœ›å€¼ï¼ˆå¯åŠ æƒï¼‰")
    target_vals, target_weights = {}, {}
    tcols = st.columns(2)
    for i, t in enumerate(target_options):
        with tcols[i % 2]:
            default_val = float(df_data[t].mean()) if t in df_data.columns else 0.0
            val = st.number_input(f"{t} ç›®æ ‡å€¼", value=default_val, key=f"rev_tval_{t}")
            w = st.number_input(f"{t} æƒé‡", value=1.0, min_value=0.0, step=0.1, key=f"rev_tw_{t}")
            target_vals[t] = float(val)
            target_weights[t] = float(w)

    # ====== é‡‡æ ·è§„æ¨¡ä¸è¾“å‡ºè®¾ç½® ======
    st.markdown("### ç”Ÿæˆä¸ç­›é€‰è®¾ç½®")
    colA, colB, colC = st.columns(3)
    with colA:
        n_samples = st.number_input("å€™é€‰é‡‡æ ·æ•°é‡", min_value=200, max_value=20000, value=2000, step=200, key="rev_nsamp")
    with colB:
        top_k = st.number_input("æ˜¾ç¤ºå‰ K ç»„", min_value=5, max_value=100, value=10, step=5, key="rev_topk")
    with colC:
        round_int = st.checkbox("ç»“æœä¸­æ•°å€¼ç‰¹å¾å››èˆäº”å…¥ä¸ºæ•´æ•°", value=True, key="rev_roundint")

    if st.button("ç”Ÿæˆæ¨èå‚æ•°ç»„åˆ"):
        # 1) å€™é€‰é‡‡æ ·
        candidates_df = _sample_candidates(int(n_samples), global_features, global_ftypes, param_limits)

        # å¯¹äºåˆ†ç±»å‹é‡Œå‡ºç° Noneï¼ˆå› ä¸ºç”¨æˆ·æŠŠå¯é€‰å€¼æ¸…ç©ºï¼‰åšå…œåº•å¡«å……
        for f in global_features:
            if global_ftypes.get(f) == "åˆ†ç±»å‹":
                if candidates_df[f].isna().any():
                    fallback = df_data[f].dropna().unique().tolist()
                    if fallback:
                        candidates_df.loc[candidates_df[f].isna(), f] = np.random.choice(fallback, size=candidates_df[f].isna().sum())

        # 2) é€ target é¢„æµ‹ï¼ˆå¤šæ¨¡å‹ï¼‰
        preds_df = _predict_with_models(candidates_df, {t: models_by_target[t] for t in target_options}, global_features)

        # 3) è¯¯å·®è®¡ç®—ï¼ˆåŠ æƒ L1ï¼‰
        err = _compute_weighted_error(preds_df, target_vals, target_weights)
        candidates_df = candidates_df.reset_index(drop=True)
        result_df = pd.concat([candidates_df, preds_df], axis=1)
        result_df["weighted_error"] = err

        # 4) é€‰å‡ºå‰ K ç»„
        best_df = result_df.sort_values("weighted_error", ascending=True).head(int(top_k)).copy()

        # æŒ‰éœ€æŠŠæ•°å€¼å‹ç‰¹å¾å–æ•´
        if round_int:
            for f in global_features:
                if global_ftypes.get(f) == "æ•°å€¼å‹":
                    best_df[f] = best_df[f].round(0).astype(int)

        # å±•ç¤º & ä¸‹è½½
        st.subheader("æ¨èçš„å‚æ•°ç»„åˆï¼ˆåŠ æƒè¯¯å·®æœ€å°çš„å‰Kç»„ï¼‰")
        st.dataframe(best_df.drop(columns=["weighted_error"]))

        csv_bytes = best_df.to_csv(index=False).encode("utf-8")
        st.download_button("ä¸‹è½½æ¨èç»“æœ CSV", data=csv_bytes, file_name="reverse_recommendations.csv", mime="text/csv")

# é¡µé¢4: ç‰¹å¾åˆ†æ
# ä¿®æ”¹ç‰¹å¾åˆ†æé¡µé¢çš„ç‰¹å¾é‡è¦æ€§å±•ç¤ºéƒ¨åˆ†
elif page == "ç‰¹å¾åˆ†æ":
    st.header("ğŸ“ˆ ç‰¹å¾é‡è¦æ€§åˆ†æ")
    
   # ä¸Šä¼ è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆjoblibï¼‰
    uploaded_model = st.file_uploader("ä¸Šä¼ è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆjoblibæ ¼å¼ï¼‰", type=['pkl', 'joblib'], key="stab_model_upload")
    if uploaded_model is not None:
        model = joblib.load(uploaded_model)
        st.success("æ¨¡å‹åŠ è½½å®Œæˆ")
        # å¦‚æœä¸Šä¼ äº†æ¨¡å‹ï¼Œä½¿ç”¨ä¸Šä¼ çš„æ¨¡å‹
        use_uploaded_model = True
    elif hasattr(st.session_state, 'model') and st.session_state.trained:
        model = st.session_state.model
        st.info("ä½¿ç”¨å½“å‰ä¼šè¯è®­ç»ƒçš„æ¨¡å‹")
        use_uploaded_model = False
    else:
        st.warning("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–ä¸Šä¼ å·²è®­ç»ƒæ¨¡å‹")
        st.stop()
    
    if not st.session_state.trained and not use_uploaded_model:
        st.warning("è¯·å…ˆåœ¨ã€Œæ¨¡å‹è®­ç»ƒã€é¡µé¢è®­ç»ƒæ¨¡å‹æˆ–ä¸Šä¼ ä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹")
    else:
        if use_uploaded_model:
            model = model
            # å°è¯•ä»ä¸Šä¼ çš„æ¨¡å‹ä¸­è·å–æ¨¡å‹ç±»å‹
            model_type = "ä¸Šä¼ çš„æ¨¡å‹"
        else:
            model = st.session_state.model
            model_type = st.session_state.get('model_type', 'æœªçŸ¥æ¨¡å‹')
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        regressor = model.named_steps['regressor']
        preprocessor = model.named_steps['preprocessor']
        
        # è·å–ç‰¹å¾åç§°
        numeric_features = [f for f in st.session_state.features if st.session_state.feature_types[f] == "æ•°å€¼å‹"]
        categorical_features = [f for f in st.session_state.features if st.session_state.feature_types[f] == "åˆ†ç±»å‹"]
        
        # å¦‚æœåŸºç‰‡ç‰¹å¾åœ¨åˆ†ç±»ç‰¹å¾ä¸­ï¼Œç¡®ä¿å®ƒä¸ä¼šè¢«é‡å¤å¤„ç†
        if hasattr(st.session_state, 'substrate_feature') and st.session_state.substrate_feature in categorical_features:
            categorical_features = [f for f in categorical_features if f != st.session_state.substrate_feature]
        
        # è·å–åˆ†ç±»ç‰¹å¾ç¼–ç åçš„åç§°
        if categorical_features:
            encoder = preprocessor.named_transformers_['cat']
            encoded_names = encoder.get_feature_names_out(categorical_features)
        else:
            encoded_names = []
        
        feature_names = numeric_features + list(encoded_names)
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è·å–ç‰¹å¾é‡è¦æ€§
        st.subheader(f"ç‰¹å¾é‡è¦æ€§åˆ†æ ({model_type})")
        if hasattr(regressor, 'feature_importances_'):
            importances = regressor.feature_importances_
            importance_type = "é‡è¦æ€§å¾—åˆ†"
        elif hasattr(regressor, 'coef_'):
            # å¯¹äºçº¿æ€§æ¨¡å‹ï¼Œä½¿ç”¨ç³»æ•°çš„ç»å¯¹å€¼ä½œä¸ºé‡è¦æ€§
            importances = np.abs(regressor.coef_)
            importance_type = "ç³»æ•°ç»å¯¹å€¼ (è¡¨ç¤ºå½±å“å¼ºåº¦)"
        else:
            st.warning("æ­¤æ¨¡å‹ç±»å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
            st.stop()
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§æ•°æ®æ¡†
        importance_df = pd.DataFrame({
            'ç‰¹å¾': feature_names,
            importance_type: importances
        }).sort_values(importance_type, ascending=False)
        
        # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾è¡¨
        st.subheader(f"ç‰¹å¾{importance_type}æ’åº")
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(x=importance_type, y='ç‰¹å¾', data=importance_df, ax=ax)
        ax.set_title(f'ç‰¹å¾{importance_type}')
        st.pyplot(fig)
        
        # æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§è¡¨æ ¼
        st.dataframe(importance_df)
        
        # å…¶ä½™éƒ¨åˆ†ä¿æŒä¸å˜...
        # éƒ¨åˆ†ä¾èµ–å›¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
        st.subheader("å‚æ•°å½±å“åˆ†æ")
        st.write("é€‰æ‹©ç‰¹å¾æŸ¥çœ‹å…¶ä¸ç›®æ ‡å˜é‡çš„å…³ç³»")
        
        # åªæ˜¾ç¤ºæ•°å€¼å‹ç‰¹å¾
        numeric_features = [f for f in st.session_state.features if st.session_state.feature_types[f] == "æ•°å€¼å‹"]
        selected_feature = st.selectbox("é€‰æ‹©ç‰¹å¾", numeric_features)
        
        if selected_feature:
            # åˆ›å»ºéƒ¨åˆ†ä¾èµ–å›¾æ•°æ®
            feature_data = st.session_state.data[selected_feature]
            grid = np.linspace(feature_data.min(), feature_data.max(), 50)

            # åªå¯¹æ•°å€¼å‹ç‰¹å¾æ±‚ä¸­ä½æ•°ï¼Œåˆ†ç±»å‹ç‰¹å¾ç”¨ä¼—æ•°
            numeric_features = [f for f in st.session_state.features if st.session_state.feature_types[f] == "æ•°å€¼å‹"]
            categorical_features = [f for f in st.session_state.features if st.session_state.feature_types[f] == "åˆ†ç±»å‹"]

            baseline_data = []
            for val in grid:
                sample = {}
                # æ•°å€¼å‹ç‰¹å¾ç”¨ä¸­ä½æ•°
                for f in numeric_features:
                    sample[f] = st.session_state.data[f].median()
                # åˆ†ç±»å‹ç‰¹å¾ç”¨ä¼—æ•°
                for f in categorical_features:
                    sample[f] = st.session_state.data[f].mode()[0]
                # å½“å‰ç‰¹å¾ç”¨éå†å€¼
                sample[selected_feature] = val
                baseline_data.append(sample)

            baseline_df = pd.DataFrame(baseline_data)

            # é¢„æµ‹
            predictions = model.predict(baseline_df)

            # ç»˜åˆ¶éƒ¨åˆ†ä¾èµ–å›¾
            fig, ax = plt.subplots()
            ax.plot(grid, predictions, 'b-')
            ax.set_xlabel(selected_feature)
            ax.set_ylabel(f"é¢„æµ‹ {st.session_state.target}")
            ax.set_title(f"{selected_feature} å¯¹ {st.session_state.target} çš„å½±å“")
            st.pyplot(fig)
                    # æ–°å¢éƒ¨åˆ†ï¼šåŒå‚æ•°å…³ç³»åˆ†æ
            st.subheader("åŒå‚æ•°å…³ç³»åˆ†æ")
            st.write("é€‰æ‹©ä¸¤ä¸ªæ•°å€¼å‹å‚æ•°è§‚å¯Ÿå®ƒä»¬çš„å…³ç³»ï¼Œå¹¶é€šè¿‡é¢œè‰²è¡¨ç¤ºç›®æ ‡å˜é‡")
            numeric_features = [f for f in st.session_state.features if st.session_state.feature_types[f] == "æ•°å€¼å‹"]
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°å€¼å‹ç‰¹å¾
            if len(numeric_features) >= 2:
                col1, col2 = st.columns(2)
                with col1:
                    x_feature = st.selectbox("é€‰æ‹©Xè½´å‚æ•°", numeric_features, key="x_feature")
                with col2:
                    # é»˜è®¤é€‰æ‹©ä¸åŒäºXè½´çš„å‚æ•°
                    y_options = [f for f in numeric_features if f != x_feature]
                    default_idx = 0 if len(y_options) > 0 else -1
                    if default_idx >= 0:
                        y_feature = st.selectbox("é€‰æ‹©Yè½´å‚æ•°", y_options, key="y_feature", index=default_idx)
                    else:
                        y_feature = st.selectbox("é€‰æ‹©Yè½´å‚æ•°", numeric_features, key="y_feature")
                
                # æ·»åŠ æ•°æ®æºé€‰æ‹©å¤é€‰æ¡†
                st.write("é€‰æ‹©è¦æ˜¾ç¤ºçš„æ•°æ®æº:")
                col_show1, col_show2, col_show3 = st.columns(3)
                with col_show1:
                    show_original = st.checkbox("åŸå§‹æ•°æ®", value=True)
                with col_show2:
                    show_augmented = st.checkbox("å¢å¼ºæ•°æ®", value=True)
                with col_show3:
                    show_predicted = st.checkbox("é¢„æµ‹å€¼", value=True)
                
                if x_feature and y_feature and x_feature != y_feature:
                    # åˆ›å»ºæ•£ç‚¹å›¾
                    fig, ax = plt.subplots(figsize=(10, 8))
                    scatter_handles = []
                    scatter_labels = []
                    
                    # 1. ç»˜åˆ¶åŸå§‹æ•°æ®
                    if show_original and st.session_state.data is not None:
                        x_original = st.session_state.data[x_feature]
                        y_original = st.session_state.data[y_feature]
                        target_original = st.session_state.data[st.session_state.target]
                        original_scatter = ax.scatter(
                            x_original, y_original, 
                            c=target_original, cmap='viridis', 
                            alpha=0.6, marker='o', s=50
                        )
                        scatter_handles.append(original_scatter)
                        scatter_labels.append('åŸå§‹æ•°æ®')
                    
                    # 2. ç»˜åˆ¶å¢å¼ºæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
                    if show_augmented and hasattr(st.session_state, 'X_train_augmented'):
                        x_augmented = st.session_state.X_train_augmented[x_feature]
                        y_augmented = st.session_state.X_train_augmented[y_feature]
                        if hasattr(st.session_state, 'y_train_augmented'):
                            target_augmented = st.session_state.y_train_augmented
                            augmented_scatter = ax.scatter(
                                x_augmented, y_augmented, 
                                c=target_augmented, cmap='plasma', 
                                alpha=0.5, marker='^', s=40
                            )
                            scatter_handles.append(augmented_scatter)
                            scatter_labels.append('å¢å¼ºæ•°æ®')
                    
                    # 3. ç»˜åˆ¶é¢„æµ‹å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
                    if show_predicted and hasattr(st.session_state, 'X_test') and hasattr(st.session_state, 'y_pred'):
                        x_pred = st.session_state.X_test[x_feature]
                        y_pred = st.session_state.X_test[y_feature]
                        target_pred = st.session_state.y_pred
                        pred_scatter = ax.scatter(
                            x_pred, y_pred, 
                            c=target_pred, cmap='cividis', 
                            alpha=0.7, marker='s', s=60
                        )
                        scatter_handles.append(pred_scatter)
                        scatter_labels.append('é¢„æµ‹å€¼')
                    
                    # æ·»åŠ é¢œè‰²æ¡å’Œå›¾ä¾‹
                    if scatter_handles:  # ç¡®ä¿æœ‰å›¾å½¢å…ƒç´ 
                        cbar = plt.colorbar(scatter_handles[0])
                        cbar.set_label(st.session_state.target)
                        ax.legend(scatter_handles, scatter_labels, loc='best')
                    
                    ax.set_xlabel(x_feature)
                    ax.set_ylabel(y_feature)
                    ax.set_title(f"{x_feature} ä¸ {y_feature} çš„å…³ç³»\n(é¢œè‰²è¡¨ç¤º {st.session_state.target})")
                    
                    # æ·»åŠ ç½‘æ ¼
                    ax.grid(True, alpha=0.3)
                    
                    st.pyplot(fig)
                    
                    # è®¡ç®—åŸå§‹æ•°æ®ç›¸å…³ç³»æ•°
                    if st.session_state.data is not None:
                        corr = np.corrcoef(st.session_state.data[x_feature], st.session_state.data[y_feature])[0, 1]
                        st.write(f"**{x_feature}** ä¸ **{y_feature}** çš„ç›¸å…³ç³»æ•°: {corr:.3f}")
                        
                        # è§£é‡Šç›¸å…³æ€§
                        if abs(corr) < 0.2:
                            strength = "å¼±"
                        elif abs(corr) < 0.6:
                            strength = "ä¸­ç­‰"
                        else:
                            strength = "å¼º"
                        
                        direction = "æ­£" if corr > 0 else "è´Ÿ"
                        st.write(f"**ç›¸å…³æ€§è§£é‡Š**: {strength}{direction}ç›¸å…³")
                elif x_feature == y_feature:
                    st.warning("è¯·é€‰æ‹©ä¸¤ä¸ªä¸åŒçš„å‚æ•°è¿›è¡Œåˆ†æ")
            else:
                st.warning("éœ€è¦è‡³å°‘ä¸¤ä¸ªæ•°å€¼å‹å‚æ•°æ‰èƒ½è¿›è¡ŒåŒå‚æ•°å…³ç³»åˆ†æ")
# é¡µé¢5: å‚æ•°åˆ†å¸ƒåˆ†æ
elif page == "å‚æ•°åˆ†å¸ƒåˆ†æ":
    st.header("ğŸ“Š å‚æ•°åˆ†å¸ƒåˆ†æ")
    
    # ä¸Šä¼ è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆjoblibï¼‰
    uploaded_model = st.file_uploader("ä¸Šä¼ è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆjoblibæ ¼å¼ï¼‰", type=['pkl', 'joblib'], key="dist_model_upload")
    if uploaded_model is not None:
        model = joblib.load(uploaded_model)
        st.success("æ¨¡å‹åŠ è½½å®Œæˆ")
        # å¦‚æœä¸Šä¼ äº†æ¨¡å‹ï¼Œä½¿ç”¨ä¸Šä¼ çš„æ¨¡å‹
        use_uploaded_model = True
    elif hasattr(st.session_state, 'model') and st.session_state.trained:
        model = st.session_state.model
        st.info("ä½¿ç”¨å½“å‰ä¼šè¯è®­ç»ƒçš„æ¨¡å‹")
        use_uploaded_model = False
    else:
        st.warning("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–ä¸Šä¼ å·²è®­ç»ƒæ¨¡å‹")
        st.stop()
    
    if st.session_state.data is None:
        st.warning("è¯·å…ˆåœ¨ã€Œæ•°æ®ä¸Šä¼ ä¸è®¾ç½®ã€é¡µé¢ä¸Šä¼ æ•°æ®")
    else:
        df = st.session_state.data
        target = st.session_state.target
        features = st.session_state.features
        feature_types = st.session_state.feature_types
        
        st.subheader("å‚æ•°å¯¹è§‚æµ‹é‡çš„åˆ†å¸ƒå½±å“åˆ†æ")
        
        # é€‰æ‹©åˆ†æå‚æ•°
        selected_feature = st.selectbox("é€‰æ‹©è¦åˆ†æçš„å‚æ•°", features)
        
        if selected_feature:
            # è·å–å‚æ•°ç±»å‹
            feature_type = feature_types[selected_feature]
            
            # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
            col1, col2 = st.columns(2)
            
            with col1:
                # æ•£ç‚¹å›¾ï¼ˆé€‚ç”¨äºæ•°å€¼å‹å‚æ•°ï¼‰
                if feature_type == "æ•°å€¼å‹":
                    # ç¡®ä¿æ•°æ®æ˜¯æ•°å€¼å‹
                    try:
                        feature_data = pd.to_numeric(df[selected_feature], errors='coerce')
                        target_data = pd.to_numeric(df[target], errors='coerce')
                        
                        # ç§»é™¤NaNå€¼
                        valid_data = pd.notna(feature_data) & pd.notna(target_data)
                        feature_data = feature_data[valid_data]
                        target_data = target_data[valid_data]
                        
                        fig, ax = plt.subplots(figsize=(8, 6))
                        ax.scatter(feature_data, target_data, alpha=0.6)
                        ax.set_xlabel(selected_feature)
                        ax.set_ylabel(target)
                        ax.set_title(f"{selected_feature} ä¸ {target} çš„å…³ç³»")
                        
                        # æ·»åŠ è¶‹åŠ¿çº¿
                        if len(feature_data) > 1:
                            z = np.polyfit(feature_data, target_data, 1)
                            p = np.poly1d(z)
                            ax.plot(feature_data, p(feature_data), "r--", alpha=0.8)
                            
                            # è®¡ç®—ç›¸å…³ç³»æ•°
                            corr = np.corrcoef(feature_data, target_data)[0, 1]
                            ax.text(0.05, 0.95, f'ç›¸å…³ç³»æ•°: {corr:.3f}', transform=ax.transAxes, 
                                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
                        
                        st.pyplot(fig)
                    except Exception as e:
                        st.error(f"åˆ›å»ºæ•£ç‚¹å›¾æ—¶å‡ºé”™: {str(e)}")
                
                # ç®±çº¿å›¾ï¼ˆé€‚ç”¨äºåˆ†ç±»å‹å‚æ•°ï¼‰
                else:
                    try:
                        fig, ax = plt.subplots(figsize=(8, 6))
                        categories = df[selected_feature].unique()
                        
                        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºæ•°æ®åˆ—è¡¨
                        data = []
                        labels = []
                        for category in categories:
                            category_data = df[df[selected_feature] == category][target]
                            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹
                            try:
                                category_data = pd.to_numeric(category_data, errors='coerce').dropna()
                                if len(category_data) > 0:  # ç¡®ä¿æœ‰æ•°æ®
                                    data.append(category_data)
                                    labels.append(str(category))
                            except:
                                # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•°å€¼å‹ï¼Œè·³è¿‡
                                continue
                        
                        if data:  # ç¡®ä¿æœ‰æœ‰æ•ˆæ•°æ®
                            ax.boxplot(data, labels=labels)
                            ax.set_xlabel(selected_feature)
                            ax.set_ylabel(target)
                            ax.set_title(f"{selected_feature} å¯¹ {target} çš„åˆ†å¸ƒå½±å“")
                            plt.xticks(rotation=45)
                            st.pyplot(fig)
                        else:
                            st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ•°å€¼æ•°æ®å¯æ˜¾ç¤º")
                    except Exception as e:
                        st.error(f"åˆ›å»ºç®±çº¿å›¾æ—¶å‡ºé”™: {str(e)}")
            
            with col2:
                # åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆé€‚ç”¨äºæ•°å€¼å‹å‚æ•°ï¼‰
                if feature_type == "æ•°å€¼å‹":
                    try:
                        # ç¡®ä¿æ•°æ®æ˜¯æ•°å€¼å‹
                        feature_data = pd.to_numeric(df[selected_feature], errors='coerce').dropna()
                        target_data = pd.to_numeric(df[target], errors='coerce').dropna()
                        
                        # é€‰æ‹©åŒºé—´æ•°
                        bins = st.slider("ç›´æ–¹å›¾åŒºé—´æ•°", 5, 50, 20)
                        
                        fig, ax = plt.subplots(figsize=(8, 6))
                        
                        # è®¡ç®—å‚æ•°çš„åˆ†ä½æ•°
                        q25, q50, q75 = np.percentile(feature_data, [25, 50, 75])
                        
                        # åˆ›å»ºå­åŒºé—´
                        low_range = feature_data <= q25
                        mid_range = (feature_data > q25) & (feature_data <= q75)
                        high_range = feature_data > q75
                        
                        # ç»˜åˆ¶åˆ†å¸ƒ
                        ax.hist(target_data[low_range], bins=bins, alpha=0.5, label=f"ä½ {selected_feature} (â‰¤{q25:.2f})")
                        ax.hist(target_data[mid_range], bins=bins, alpha=0.5, label=f"ä¸­ {selected_feature} ({q25:.2f}-{q75:.2f})")
                        ax.hist(target_data[high_range], bins=bins, alpha=0.5, label=f"é«˜ {selected_feature} (>{q75:.2f})")
                        
                        ax.set_xlabel(target)
                        ax.set_ylabel("é¢‘æ•°")
                        ax.set_title(f"ä¸åŒ {selected_feature} åŒºé—´çš„ {target} åˆ†å¸ƒ")
                        ax.legend()
                        st.pyplot(fig)
                    except Exception as e:
                        st.error(f"åˆ›å»ºç›´æ–¹å›¾æ—¶å‡ºé”™: {str(e)}")
                
                # å°æç´å›¾ï¼ˆé€‚ç”¨äºåˆ†ç±»å‹å‚æ•°ï¼‰
                else:
                    try:
                        fig, ax = plt.subplots(figsize=(8, 6))
                        
                        categories = df[selected_feature].unique()
                        
                        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºæ•°æ®åˆ—è¡¨
                        data = []
                        labels = []
                        for category in categories:
                            category_data = df[df[selected_feature] == category][target]
                            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹
                            try:
                                category_data = pd.to_numeric(category_data, errors='coerce').dropna()
                                if len(category_data) > 0:  # ç¡®ä¿æœ‰æ•°æ®
                                    data.append(category_data)
                                    labels.append(str(category))
                            except:
                                # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•°å€¼å‹ï¼Œè·³è¿‡
                                continue
                        
                        if data:  # ç¡®ä¿æœ‰æœ‰æ•ˆæ•°æ®
                            # åˆ›å»ºå°æç´å›¾
                            parts = ax.violinplot(data, showmeans=True, showmedians=True)
                            
                            # è®¾ç½®é¢œè‰²
                            for pc in parts['bodies']:
                                pc.set_facecolor('lightblue')
                                pc.set_alpha(0.6)
                            
                            ax.set_xlabel(selected_feature)
                            ax.set_ylabel(target)
                            ax.set_title(f"{selected_feature} å¯¹ {target} çš„åˆ†å¸ƒå½±å“")
                            ax.set_xticks(range(1, len(labels) + 1))
                            ax.set_xticklabels(labels, rotation=45)
                            st.pyplot(fig)
                        else:
                            st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ•°å€¼æ•°æ®å¯æ˜¾ç¤º")
                    except Exception as e:
                        st.error(f"åˆ›å»ºå°æç´å›¾æ—¶å‡ºé”™: {str(e)}")
            
            # ç»Ÿè®¡åˆ†æ
            st.subheader("ç»Ÿè®¡åˆ†æ")
            
            if feature_type == "æ•°å€¼å‹":
                try:
                    # ç¡®ä¿æ•°æ®æ˜¯æ•°å€¼å‹
                    feature_data = pd.to_numeric(df[selected_feature], errors='coerce').dropna()
                    target_data = pd.to_numeric(df[target], errors='coerce').dropna()
                    
                    # è®¡ç®—ç›¸å…³ç³»æ•°å’Œpå€¼
                    if len(feature_data) > 1 and len(target_data) > 1:
                        corr, p_value = stats.pearsonr(feature_data, target_data)
                        st.write(f"**{selected_feature}** ä¸ **{target}** çš„ Pearson ç›¸å…³ç³»æ•°: {corr:.3f} (på€¼: {p_value:.3e})")
                        
                        # è§£é‡Šç›¸å…³æ€§å¼ºåº¦
                        if abs(corr) < 0.3:
                            strength = "å¼±ç›¸å…³"
                        elif abs(corr) < 0.7:
                            strength = "ä¸­ç­‰ç›¸å…³"
                        else:
                            strength = "å¼ºç›¸å…³"
                        
                        direction = "æ­£" if corr > 0 else "è´Ÿ"
                        st.write(f"**ç›¸å…³æ€§è§£é‡Š**: {strength}{direction}ç›¸å…³")
                    else:
                        st.warning("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§")
                except Exception as e:
                    st.error(f"è®¡ç®—ç›¸å…³æ€§æ—¶å‡ºé”™: {str(e)}")
            
            else:
                try:
                    # å¯¹åˆ†ç±»å˜é‡è¿›è¡ŒANOVAåˆ†æ
                    categories = df[selected_feature].unique()
                    group_data = []
                    valid_categories = []
                    
                    for category in categories:
                        category_data = df[df[selected_feature] == category][target]
                        # å°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹
                        try:
                            category_data = pd.to_numeric(category_data, errors='coerce').dropna()
                            if len(category_data) > 0:  # ç¡®ä¿æœ‰æ•°æ®
                                group_data.append(category_data)
                                valid_categories.append(category)
                        except:
                            # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•°å€¼å‹ï¼Œè·³è¿‡
                            continue
                    
                    if len(group_data) >= 2:
                        f_stat, p_value = stats.f_oneway(*group_data)
                        st.write(f"ANOVA åˆ†æ Fç»Ÿè®¡é‡: {f_stat:.3f} (på€¼: {p_value:.3e})")
                        
                        if p_value < 0.05:
                            st.write("**ç»Ÿè®¡æ˜¾è‘—æ€§**: ä¸åŒç»„åˆ«ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ (p < 0.05)")
                        else:
                            st.write("**ç»Ÿè®¡æ˜¾è‘—æ€§**: ä¸åŒç»„åˆ«ä¹‹é—´æ²¡æœ‰æ˜¾è‘—å·®å¼‚ (p â‰¥ 0.05)")
                    else:
                        st.warning("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡ŒANOVAåˆ†æ")
                    
                    # æ˜¾ç¤ºå„ç»„çš„æè¿°æ€§ç»Ÿè®¡
                    st.write("**å„ç»„æè¿°æ€§ç»Ÿè®¡**:")
                    desc_stats = []
                    
                    for category in valid_categories:
                        category_data = df[df[selected_feature] == category][target]
                        # å°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹
                        try:
                            category_data = pd.to_numeric(category_data, errors='coerce').dropna()
                            if len(category_data) > 0:  # ç¡®ä¿æœ‰æ•°æ®
                                desc_stats.append({
                                    'ç±»åˆ«': str(category),
                                    'æ•°é‡': len(category_data),
                                    'å‡å€¼': category_data.mean(),
                                    'æ ‡å‡†å·®': category_data.std(),
                                    'æœ€å°å€¼': category_data.min(),
                                    'æœ€å¤§å€¼': category_data.max()
                                })
                        except:
                            # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•°å€¼å‹ï¼Œè·³è¿‡
                            continue
                    
                    if desc_stats:
                        desc_df = pd.DataFrame(desc_stats)
                        st.dataframe(desc_df.round(3))
                    else:
                        st.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ•°å€¼æ•°æ®å¯æ˜¾ç¤º")
                except Exception as e:
                    st.error(f"è¿›è¡Œç»Ÿè®¡åˆ†ææ—¶å‡ºé”™: {str(e)}")

# é¡µé¢6: æ¨¡å‹ç¨³å®šæ€§éªŒè¯
elif page == "æ¨¡å‹ç¨³å®šæ€§éªŒè¯":
    st.header("ğŸ” æ¨¡å‹ç¨³å®šæ€§éªŒè¯")
    
    # ä¸Šä¼ è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆjoblibï¼‰
    uploaded_model = st.file_uploader("ä¸Šä¼ è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆjoblibæ ¼å¼ï¼‰", type=['pkl', 'joblib'], key="stab_model_upload")
    if uploaded_model is not None:
        model = joblib.load(uploaded_model)
        st.success("æ¨¡å‹åŠ è½½å®Œæˆ")
        # å¦‚æœä¸Šä¼ äº†æ¨¡å‹ï¼Œä½¿ç”¨ä¸Šä¼ çš„æ¨¡å‹
        use_uploaded_model = True
    elif hasattr(st.session_state, 'model') and st.session_state.trained:
        model = st.session_state.model
        st.info("ä½¿ç”¨å½“å‰ä¼šè¯è®­ç»ƒçš„æ¨¡å‹")
        use_uploaded_model = False
    else:
        st.warning("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–ä¸Šä¼ å·²è®­ç»ƒæ¨¡å‹")
        st.stop()
    
    st.info("""
    æ¨¡å‹ç¨³å®šæ€§éªŒè¯é€šè¿‡å¤šç§æ–¹æ³•è¯„ä¼°æ¨¡å‹çš„å¯é æ€§:
    - **äº¤å‰éªŒè¯**: å¤šæ¬¡éšæœºåˆ’åˆ†æ•°æ®ï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„ä¸€è‡´æ€§
    - **è‡ªåŠ©æ³•**: é€šè¿‡é‡é‡‡æ ·è¯„ä¼°æ¨¡å‹å¯¹æ•°æ®å˜åŒ–çš„æ•æ„Ÿæ€§
    """)
    
    # é€‰æ‹©éªŒè¯æ–¹æ³•
    validation_method = st.selectbox(
        "é€‰æ‹©ç¨³å®šæ€§éªŒè¯æ–¹æ³•",
        ["äº¤å‰éªŒè¯", "è‡ªåŠ©æ³•"]
    )
    
    if validation_method == "äº¤å‰éªŒè¯":
        st.subheader("äº¤å‰éªŒè¯ç¨³å®šæ€§åˆ†æ")
        
        n_folds = st.slider("äº¤å‰éªŒè¯æŠ˜æ•°", 3, 10, 5)
        n_repeats = st.slider("é‡å¤æ¬¡æ•°", 1, 10, 3)
        
        if st.button("å¼€å§‹äº¤å‰éªŒè¯"):
            with st.spinner("æ­£åœ¨è¿›è¡Œäº¤å‰éªŒè¯..."):
                from sklearn.model_selection import cross_val_score, RepeatedKFold
                
                # è·å–æ•°æ®å’Œæ¨¡å‹
                df = st.session_state.data
                features = st.session_state.features
                target = st.session_state.target
                feature_types = st.session_state.feature_types
                
                # æ ¹æ®åŸºç‰‡é€‰æ‹©è¿‡æ»¤æ•°æ®
                if st.session_state.substrate_option == "ç‰¹å®šåŸºç‰‡" and st.session_state.selected_substrate:
                    substrate_feature = st.session_state.substrate_feature
                    df = df[df[substrate_feature] == st.session_state.selected_substrate]
                
                X = df[features]
                y = df[target]
                
                # æ„å»ºé¢„å¤„ç†ç®¡é“
                numeric_features = [f for f in features if feature_types[f] == "æ•°å€¼å‹"]
                categorical_features = [f for f in features if feature_types[f] == "åˆ†ç±»å‹"]
                
                if hasattr(st.session_state, 'substrate_feature') and st.session_state.substrate_feature in categorical_features:
                    categorical_features = [f for f in categorical_features if f != st.session_state.substrate_feature]
                
                preprocessor = ColumnTransformer([
                    ('num', StandardScaler(), numeric_features),
                    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
                ])
                
                # è·å–æ¨¡å‹ç±»å‹å’Œå‚æ•°
                model_type = st.session_state.model_type
                model = st.session_state.model
                regressor = model.named_steps['regressor']
                
                # åˆ›å»ºå®Œæ•´ç®¡é“
                full_model = Pipeline([
                    ('preprocessor', preprocessor),
                    ('regressor', regressor)
                ])
                
                # æ‰§è¡Œé‡å¤KæŠ˜äº¤å‰éªŒè¯
                cv = RepeatedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=42)
                cv_scores = cross_val_score(full_model, X, y, cv=cv, scoring='r2')
                
                # è®¡ç®—ç»Ÿè®¡é‡
                mean_score = np.mean(cv_scores)
                std_score = np.std(cv_scores)
                cv_range = np.ptp(cv_scores)  # æå·®
                
                # æ˜¾ç¤ºç»“æœ
                st.subheader("äº¤å‰éªŒè¯ç»“æœ")
                col1, col2, col3 = st.columns(3)
                col1.metric("å¹³å‡ RÂ²", f"{mean_score:.4f}")
                col2.metric("æ ‡å‡†å·®", f"{std_score:.4f}")
                col3.metric("æå·®", f"{cv_range:.4f}")
                
                # ç»˜åˆ¶ç»“æœåˆ†å¸ƒ
                fig, ax = plt.subplots(figsize=(10, 6))
                ax.hist(cv_scores, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
                ax.axvline(mean_score, color='red', linestyle='--', label=f'å¹³å‡å€¼: {mean_score:.4f}')
                ax.set_xlabel('Ræ–¹ åˆ†æ•°')
                ax.set_ylabel('é¢‘æ•°')
                ax.set_title(f'äº¤å‰éªŒè¯ Ræ–¹ åˆ†æ•°åˆ†å¸ƒ ({n_folds}æŠ˜ Ã— {n_repeats}æ¬¡)')
                ax.legend()
                st.pyplot(fig)
                
                # ç¨³å®šæ€§è¯„ä¼°
                st.subheader("ç¨³å®šæ€§è¯„ä¼°")
                if std_score < 0.05:
                    stability = "é«˜ç¨³å®šæ€§"
                    stability_color = "green"
                    stability_desc = "æ¨¡å‹åœ¨ä¸åŒæ•°æ®å­é›†ä¸Šè¡¨ç°ä¸€è‡´ï¼Œç¨³å®šæ€§è‰¯å¥½"
                elif std_score < 0.1:
                    stability = "ä¸­ç­‰ç¨³å®šæ€§"
                    stability_color = "orange"
                    stability_desc = "æ¨¡å‹è¡¨ç°æœ‰ä¸€å®šæ³¢åŠ¨ï¼Œä½†æ€»ä½“ç¨³å®š"
                else:
                    stability = "ä½ç¨³å®šæ€§"
                    stability_color = "red"
                    stability_desc = "æ¨¡å‹è¡¨ç°æ³¢åŠ¨è¾ƒå¤§ï¼Œå¯èƒ½å¯¹æ•°æ®æ•æ„Ÿæˆ–è¿‡æ‹Ÿåˆ"
                
                st.markdown(f"<h4 style='color:{stability_color}'>ç¨³å®šæ€§: {stability}</h4>", unsafe_allow_html=True)
                st.write(stability_desc)
                
                # æä¾›æ”¹è¿›å»ºè®®
                st.subheader("æ”¹è¿›å»ºè®®")
                if stability == "ä½ç¨³å®šæ€§":
                    st.write("""
                    - å°è¯•å¢åŠ æ•°æ®é‡æˆ–ä½¿ç”¨æ•°æ®å¢å¼º
                    - ç®€åŒ–æ¨¡å‹å¤æ‚åº¦ï¼ˆå¦‚å‡å°‘æ ‘çš„æœ€å¤§æ·±åº¦ï¼‰
                    - å¢åŠ æ­£åˆ™åŒ–å¼ºåº¦
                    - æ£€æŸ¥ç‰¹å¾é€‰æ‹©æ˜¯å¦åˆç†
                    - è€ƒè™‘ä½¿ç”¨æ›´ç¨³å®šçš„ç®—æ³•ï¼ˆå¦‚çº¿æ€§æ¨¡å‹ï¼‰
                    """)
                elif stability == "ä¸­ç­‰ç¨³å®šæ€§":
                    st.write("""
                    - å¯ä»¥å°è¯•è°ƒæ•´æ¨¡å‹è¶…å‚æ•°
                    - è€ƒè™‘ä½¿ç”¨é›†æˆæ–¹æ³•æé«˜ç¨³å®šæ€§
                    - æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼å½±å“æ¨¡å‹
                    """)
                else:
                    st.write("""
                    - æ¨¡å‹ç¨³å®šæ€§è‰¯å¥½ï¼Œå¯ä»¥æ”¾å¿ƒä½¿ç”¨
                    - å¯ä»¥è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½
                    """)
    
    elif validation_method == "è‡ªåŠ©æ³•":
        st.subheader("è‡ªåŠ©æ³•ç¨³å®šæ€§åˆ†æ")
        
        n_iterations = st.slider("è‡ªåŠ©æ³•è¿­ä»£æ¬¡æ•°", 10, 100, 50)
        sample_ratio = st.slider("æ¯æ¬¡é‡‡æ ·çš„æ¯”ä¾‹", 0.5, 0.95, 0.8)
        
        if st.button("å¼€å§‹è‡ªåŠ©æ³•éªŒè¯"):
            with st.spinner("æ­£åœ¨è¿›è¡Œè‡ªåŠ©æ³•éªŒè¯..."):
                # è·å–æ•°æ®å’Œæ¨¡å‹
                df = st.session_state.data
                features = st.session_state.features
                target = st.session_state.target
                
                X = df[features]
                y = df[target]
                
                # å­˜å‚¨æ¯æ¬¡è¿­ä»£çš„å¾—åˆ†
                bootstrap_scores = []
                
                # è¿›åº¦æ¡
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i in range(n_iterations):
                    # æ›´æ–°è¿›åº¦
                    progress = (i + 1) / n_iterations
                    progress_bar.progress(progress)
                    status_text.text(f"æ­£åœ¨è¿›è¡Œç¬¬ {i+1}/{n_iterations} æ¬¡è¿­ä»£...")
                    
                    # è‡ªåŠ©æ³•é‡‡æ ·
                    n_samples = int(len(X) * sample_ratio)
                    indices = np.random.choice(len(X), n_samples, replace=True)
                    X_bootstrap = X.iloc[indices]
                    y_bootstrap = y.iloc[indices]
                    
                    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
                    X_train, X_test, y_train, y_test = train_test_split(
                        X_bootstrap, y_bootstrap, test_size=0.2, random_state=42
                    )
                    
                    # å…‹éš†åŸå§‹æ¨¡å‹
                    from sklearn.base import clone
                    model_clone = clone(st.session_state.model)
                    
                    # è®­ç»ƒå’Œè¯„ä¼°
                    model_clone.fit(X_train, y_train)
                    y_pred = model_clone.predict(X_test)
                    score = r2_score(y_test, y_pred)
                    bootstrap_scores.append(score)
                
                # å®Œæˆè¿›åº¦
                progress_bar.empty()
                status_text.empty()
                
                # è®¡ç®—ç»Ÿè®¡é‡
                mean_score = np.mean(bootstrap_scores)
                std_score = np.std(bootstrap_scores)
                ci_low = np.percentile(bootstrap_scores, 2.5)
                ci_high = np.percentile(bootstrap_scores, 97.5)
                
                # æ˜¾ç¤ºç»“æœ
                st.subheader("è‡ªåŠ©æ³•éªŒè¯ç»“æœ")
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("å¹³å‡ RÂ²", f"{mean_score:.4f}")
                col2.metric("æ ‡å‡†å·®", f"{std_score:.4f}")
                col3.metric("95% CI ä¸‹é™", f"{ci_low:.4f}")
                col4.metric("95% CI ä¸Šé™", f"{ci_high:.4f}")
                
                # ç»˜åˆ¶ç»“æœåˆ†å¸ƒ
                fig, ax = plt.subplots(figsize=(10, 6))
                ax.hist(bootstrap_scores, bins=15, alpha=0.7, color='lightgreen', edgecolor='black')
                ax.axvline(mean_score, color='red', linestyle='--', label=f'å¹³å‡å€¼: {mean_score:.4f}')
                ax.axvline(ci_low, color='orange', linestyle=':', label=f'95% CIä¸‹é™: {ci_low:.4f}')
                ax.axvline(ci_high, color='orange', linestyle=':', label=f'95% CIä¸Šé™: {ci_high:.4f}')
                ax.set_xlabel('Ræ–¹ åˆ†æ•°')
                ax.set_ylabel('é¢‘æ•°')
                ax.set_title(f'è‡ªåŠ©æ³• Ræ–¹ åˆ†æ•°åˆ†å¸ƒ ({n_iterations}æ¬¡è¿­ä»£)')
                ax.legend()
                st.pyplot(fig)
                
                # ç¨³å®šæ€§è¯„ä¼°
                st.subheader("ç¨³å®šæ€§è¯„ä¼°")
                score_range = ci_high - ci_low
                if score_range < 0.1:
                    stability = "é«˜ç¨³å®šæ€§"
                    stability_color = "green"
                    stability_desc = "æ¨¡å‹åœ¨ä¸åŒæ•°æ®å­é›†ä¸Šè¡¨ç°ä¸€è‡´ï¼Œç½®ä¿¡åŒºé—´çª„"
                elif score_range < 0.2:
                    stability = "ä¸­ç­‰ç¨³å®šæ€§"
                    stability_color = "orange"
                    stability_desc = "æ¨¡å‹è¡¨ç°æœ‰ä¸€å®šæ³¢åŠ¨ï¼Œä½†ç½®ä¿¡åŒºé—´åˆç†"
                else:
                    stability = "ä½ç¨³å®šæ€§"
                    stability_color = "red"
                    stability_desc = "æ¨¡å‹è¡¨ç°æ³¢åŠ¨è¾ƒå¤§ï¼Œç½®ä¿¡åŒºé—´å®½"
                
                st.markdown(f"<h4 style='color:{stability_color}'>ç¨³å®šæ€§: {stability}</h4>", unsafe_allow_html=True)
                st.write(stability_desc)
                st.write(f"95% ç½®ä¿¡åŒºé—´å®½åº¦: {score_range:.4f}")
                st.sidebar.markdown("---")
st.sidebar.info(
    """
    **ä½¿ç”¨è¯´æ˜:** 
    1. åœ¨ã€Œå®éªŒæ•°æ®æ’å€¼å¢å¼ºã€é¡µé¢è¿›è¡ŒåŸºäºç‰©ç†ç›´è§‰çš„æ•°æ®å¢å¼ºï¼ˆå¯é€‰ï¼‰
    2. åœ¨ã€Œæ•°æ®ä¸Šä¼ ä¸è®¾ç½®ã€é¡µé¢ä¸Šä¼ CSVæ•°æ®å¹¶è®¾ç½®ç‰¹å¾
    3. åœ¨ã€Œæ¨¡å‹è®­ç»ƒã€é¡µé¢è®­ç»ƒ"éšæœºæ£®æ—å›å½’", "çº¿æ€§å›å½’", "Ridgeå›å½’", "Lassoå›å½’", "æ”¯æŒå‘é‡å›å½’(SVR)"æ¨¡å‹
    4. åœ¨ã€Œå‚æ•°é¢„æµ‹ã€é¡µé¢è¾“å…¥å‚æ•°è¿›è¡Œé¢„æµ‹
    5. åœ¨ã€Œç‰¹å¾åˆ†æã€é¡µé¢æŸ¥çœ‹å„å‚æ•°çš„é‡è¦æ€§ä»¥åŠå‚æ•°é—´å…³ç³»
    6. åœ¨ã€Œå‚æ•°åˆ†å¸ƒåˆ†æã€é¡µé¢æŸ¥çœ‹å‚æ•°å¯¹è§‚æµ‹é‡çš„åˆ†å¸ƒå½±å“
    7. åœ¨ã€Œæ¨¡å‹ç¨³å®šæ€§éªŒè¯ã€é¡µé¢è¯„ä¼°æ¨¡å‹çš„ç¨³å®šæ€§
    8. æ‰€æœ‰ç»“æœå‡å¯ä¸‹è½½ï¼Œæ–¹ä¾¿åç»­åˆ†æ
    """
)